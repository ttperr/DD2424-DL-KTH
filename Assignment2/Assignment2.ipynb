{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "> Tristan PERROT\n",
    "\n",
    "## Utils\n",
    "\n",
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "DATASET_PATH = '../Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(filename):\n",
    "    \"\"\" Copied from the dataset website \"\"\"\n",
    "    import pickle\n",
    "    with open(DATASET_PATH + filename, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\" Read the data from the file \"\"\"\n",
    "    data = load_batch(filename)\n",
    "    X = data[b'data'].T / 255\n",
    "    y = np.array(data[b'labels'])\n",
    "    Y = np.zeros((10, X.shape[1]))\n",
    "    for i in range(y.shape[0]):\n",
    "        Y[y[i], i] = 1\n",
    "    return X, Y, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relative_error(grad_analytical, grad_numerical, eps=1e-9):\n",
    "    \"\"\" Compute the relative error between the analytical and numerical gradients \"\"\"\n",
    "    return torch.max(\n",
    "        torch.abs(grad_analytical - grad_numerical) / torch.clamp(torch.abs(grad_analytical) + torch.abs(grad_numerical) + eps, min=eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    def __init__(self, x_train, x_val, x_test, y_train, y_val, y_test, y_hot_train, y_hot_val, y_hot_test):\n",
    "        self.x_train = torch.tensor(x_train)\n",
    "        self.x_val = torch.tensor(x_val)\n",
    "        self.x_test = torch.tensor(x_test)\n",
    "        self.y_train = torch.tensor(y_train)\n",
    "        self.y_val = torch.tensor(y_val)\n",
    "        self.y_test = torch.tensor(y_test)\n",
    "        self.y_hot_train = torch.tensor(y_hot_train)\n",
    "        self.y_hot_val = torch.tensor(y_hot_val)\n",
    "        self.y_hot_test = torch.tensor(y_hot_test)\n",
    "\n",
    "        self.eta = 0.001\n",
    "        self.lambd = 0.0\n",
    "\n",
    "    def preprocess(self):\n",
    "        mean_X = torch.mean(self.x_train, axis=1).reshape(-1, 1)\n",
    "        std_X = torch.std(self.x_train, axis=1).reshape(-1, 1)\n",
    "        self.x_train = (self.x_train - mean_X) / std_X\n",
    "        self.x_val = (self.x_val - mean_X) / std_X\n",
    "        self.x_test = (self.x_test - mean_X) / std_X\n",
    "\n",
    "    def initialize_weights(self, hidden_size):\n",
    "        features = self.x_train.shape[0]\n",
    "        outputs = self.y_train.shape[0]\n",
    "\n",
    "        self.W1 = torch.randn(hidden_size, features,\n",
    "                              dtype=self.x_train.dtype) / math.sqrt(features)\n",
    "        self.b1 = torch.zeros(hidden_size, 1, dtype=self.x_train.dtype)\n",
    "        self.W2 = torch.randn(outputs, hidden_size,\n",
    "                              dtype=self.x_train.dtype) / math.sqrt(hidden_size)\n",
    "        self.b2 = torch.zeros(outputs, 1, dtype=self.x_train.dtype)\n",
    "        self.W1.requires_grad = True\n",
    "        self.b1.requires_grad = True\n",
    "        self.W2.requires_grad = True\n",
    "        self.b2.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z1 = self.W1 @ x + self.b1\n",
    "        self.h = torch.relu(self.z1)\n",
    "        self.z2 = self.W2 @ self.h + self.b2\n",
    "        self.p = torch.softmax(self.z2, dim=0)\n",
    "        return self.p\n",
    "    \n",
    "    def forward_test(self, x, W1, b1, W2, b2):\n",
    "        z1 = W1 @ x + b1\n",
    "        h = torch.relu(z1)\n",
    "        z2 = W2 @ h + b2\n",
    "        p = torch.softmax(z2, dim=0)\n",
    "        return p\n",
    "\n",
    "    def cost(self, y, W1, W2):\n",
    "        return -torch.mean(y.T @ torch.log(self.p)) + self.lambd * (torch.sum(W1 ** 2) + torch.sum(W2 ** 2))\n",
    "\n",
    "    def cost_grad(self, x, y):\n",
    "        m = x.shape[1]\n",
    "        self.p = self.forward(x)\n",
    "        dZ2 = self.p - y\n",
    "        dW2 = dZ2 @ self.h.T / m + 2 * self.lambd * self.W2\n",
    "        db2 = torch.sum(dZ2, axis=1, keepdim=True) / m\n",
    "        dH = self.W2.T @ dZ2\n",
    "        dZ1 = dH * (self.z1 > 0)\n",
    "        dW1 = dZ1 @ x.T / m + 2 * self.lambd * self.W1\n",
    "        db1 = torch.sum(dZ1, axis=1, keepdim=True) / m\n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def compute_grads_num(self, h):\n",
    "        dW1 = torch.zeros(self.W1.shape)\n",
    "        dW2 = torch.zeros(self.W2.shape)\n",
    "        db1 = torch.zeros(self.b1.shape)\n",
    "        db2 = torch.zeros(self.b2.shape)\n",
    "\n",
    "        W1_try = self.W1.clone()\n",
    "        W2_try = self.W2.clone()\n",
    "        b1_try = self.b1.clone()\n",
    "        b2_try = self.b2.clone()\n",
    "\n",
    "        self.forward(self.x_train)\n",
    "        c = self.cost(self.y_train, self.W1, self.W2)\n",
    "        \n",
    "        for i in range(self.W1.shape[0]):\n",
    "            for j in range(self.W1.shape[1]):\n",
    "                W1_try[i, j] = self.W1[i, j] - h\n",
    "                self.forward_test(self.x_train, W1_try, self.b1, self.W2, self.b2)\n",
    "                c1 = self.cost(self.y_train, W1_try, self.W2)\n",
    "\n",
    "                W1_try[i, j] = self.W1[i, j] + h\n",
    "                self.forward_test(self.x_train, W1_try, self.b1, self.W2, self.b2)\n",
    "                c2 = self.cost(self.y_train, W1_try, self.W2)\n",
    "\n",
    "                dW1[i, j] = (c2 - c1) / (2 * h)\n",
    "        \n",
    "        for i in range(self.W2.shape[0]):\n",
    "            for j in range(self.W2.shape[1]):\n",
    "                W2_try[i, j] = self.W2[i, j] - h\n",
    "                self.forward_test(self.x_train, self.W1, self.b1, W2_try, self.b2)\n",
    "                c1 = self.cost(self.y_train, self.W1, W2_try)\n",
    "\n",
    "                W2_try[i, j] = self.W2[i, j] + h\n",
    "                self.forward_test(self.x_train, self.W1, self.b1, W2_try, self.b2)\n",
    "                c2 = self.cost(self.y_train, self.W1, W2_try)\n",
    "\n",
    "                dW2[i, j] = (c2 - c1) / (2 * h)\n",
    "\n",
    "        for i in range(self.b1.shape[0]):\n",
    "            b1_try[i] = self.b1[i] - h\n",
    "            self.forward_test(self.x_train, self.W1, b1_try, self.W2, self.b2)\n",
    "            c1 = self.cost(self.y_train, self.W1, self.W2)\n",
    "\n",
    "            b1_try[i] = self.b1[i] + h\n",
    "            self.forward_test(self.x_train, self.W1, b1_try, self.W2, self.b2)\n",
    "            c2 = self.cost(self.y_train, self.W1, self.W2)\n",
    "\n",
    "            db1[i] = (c2 - c1) / (2 * h)\n",
    "\n",
    "        for i in range(self.b2.shape[0]):\n",
    "            b2_try[i] = self.b2[i] - h\n",
    "            self.forward_test(self.x_train, self.W1, self.b1, self.W2, b2_try)\n",
    "            c1 = self.cost(self.y_train, self.W1, self.W2)\n",
    "\n",
    "            b2_try[i] = self.b2[i] + h\n",
    "            self.forward_test(self.x_train, self.W1, self.b1, self.W2, b2_try)\n",
    "            c2 = self.cost(self.y_train, self.W1, self.W2)\n",
    "\n",
    "            db2[i] = (c2 - c1) / (2 * h)\n",
    "\n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def check_grad(self):\n",
    "        dW1, db1, dW2, db2 = self.cost_grad(self.x_train, self.y_hot_train)\n",
    "        cost = self.cost(self.y_train, self.W1, self.W2)\n",
    "        cost.backward()\n",
    "        grad_W1 = self.W1.grad\n",
    "        grad_b1 = self.b1.grad\n",
    "        grad_W2 = self.W2.grad\n",
    "        grad_b2 = self.b2.grad\n",
    "\n",
    "        print('Between analytical and torch gradients:')\n",
    "        print('dW1:', dW1.shape, 'grad_W1:', grad_W1.shape)\n",
    "        print('db1:', db1.shape, 'grad_b1:', grad_b1.shape)\n",
    "        print('dW2:', dW2.shape, 'grad_W2:', grad_W2.shape)\n",
    "        print('db2:', db2.shape, 'grad_b2:', grad_b2.shape)\n",
    "        print('Difference for W1:', torch.max(torch.abs(dW1 - grad_W1)))\n",
    "        print('Difference for b1:', torch.max(torch.abs(db1 - grad_b1)))\n",
    "        print('Difference for W2:', torch.max(torch.abs(dW2 - grad_W2)))\n",
    "        print('Difference for b2:', torch.max(torch.abs(db2 - grad_b2)))\n",
    "        print('Relative error for W1:', compute_relative_error(dW1, grad_W1))\n",
    "        print('Relative error for b1:', compute_relative_error(db1, grad_b1))\n",
    "        print('Relative error for W2:', compute_relative_error(dW2, grad_W2))\n",
    "        print('Relative error for b2:', compute_relative_error(db2, grad_b2))\n",
    "\n",
    "        dW1_num, db1_num, dW2_num, db2_num = self.compute_grads_num(1e-9)\n",
    "\n",
    "        print('\\nBetween analytical and numerical gradients:')\n",
    "        print('Difference for W1:', torch.max(torch.abs(dW1 - dW1_num)))\n",
    "        print('Difference for b1:', torch.max(torch.abs(db1 - db1_num)))\n",
    "        print('Difference for W2:', torch.max(torch.abs(dW2 - dW2_num)))\n",
    "        print('Difference for b2:', torch.max(torch.abs(db2 - db2_num)))\n",
    "        print('Relative error for W1:', compute_relative_error(dW1, dW1_num))\n",
    "        print('Relative error for b1:', compute_relative_error(db1, db1_num))\n",
    "        print('Relative error for W2:', compute_relative_error(dW2, dW2_num))\n",
    "        print('Relative error for b2:', compute_relative_error(db2, db2_num))\n",
    "\n",
    "        print('\\nBetween torch and numerical gradients:')\n",
    "        print('Difference for W1:', torch.max(torch.abs(grad_W1 - dW1_num)))\n",
    "        print('Difference for b1:', torch.max(torch.abs(grad_b1 - db1_num)))\n",
    "        print('Difference for W2:', torch.max(torch.abs(grad_W2 - dW2_num)))\n",
    "        print('Difference for b2:', torch.max(torch.abs(grad_b2 - db2_num)))\n",
    "        print('Relative error for W1:', compute_relative_error(grad_W1, dW1_num))\n",
    "        print('Relative error for b1:', compute_relative_error(grad_b1, db1_num))\n",
    "        print('Relative error for W2:', compute_relative_error(grad_W2, dW2_num))\n",
    "        print('Relative error for b2:', compute_relative_error(grad_b2, db2_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 10) (10, 10) (10,)\n",
      "(50, 10) (10, 10) (10,)\n",
      "(50, 10) (10, 10) (10,)\n",
      "torch.Size([5, 50]) torch.Size([5, 1]) torch.Size([10, 5]) torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, y_train = load_data('data_batch_1')\n",
    "X_val, Y_val, y_val = load_data('data_batch_2')\n",
    "X_test, Y_test, y_test = load_data('test_batch')\n",
    "\n",
    "n = 10\n",
    "dim = 50\n",
    "m = 5\n",
    "\n",
    "X_train = X_train[:dim, :n]\n",
    "Y_train = Y_train[:, :n]\n",
    "y_train = y_train[:n]\n",
    "X_val = X_val[:dim, :n]\n",
    "Y_val = Y_val[:, :n]\n",
    "y_val = y_val[:n]\n",
    "X_test = X_test[:dim, :n]\n",
    "Y_test = Y_test[:, :n]\n",
    "y_test = y_test[:n]\n",
    "\n",
    "self = Classifier(X_train, X_val, X_test, Y_train,\n",
    "                        Y_val, Y_test, y_train, y_val, y_test)\n",
    "self.preprocess()\n",
    "self.initialize_weights(m)\n",
    "print(X_train.shape, Y_train.shape, y_train.shape)\n",
    "print(X_val.shape, Y_val.shape, y_val.shape)\n",
    "print(X_test.shape, Y_test.shape, y_test.shape)\n",
    "print(self.W1.shape, self.b1.shape,\n",
    "      self.W2.shape, self.b2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between analytical and torch gradients:\n",
      "dW1: torch.Size([5, 50]) grad_W1: torch.Size([5, 50])\n",
      "db1: torch.Size([5, 1]) grad_b1: torch.Size([5, 1])\n",
      "dW2: torch.Size([10, 5]) grad_W2: torch.Size([10, 5])\n",
      "db2: torch.Size([10, 1]) grad_b2: torch.Size([10, 1])\n",
      "Difference for W1: tensor(4.7554, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Difference for b1: tensor(5.3112, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Difference for W2: tensor(2.7605, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Difference for b2: tensor(5.0000, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Relative error for W1: tensor(1.0000, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Relative error for b1: tensor(1.0000, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Relative error for W2: tensor(1.0000, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Relative error for b2: tensor(1.0000, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "\n",
      "Between analytical and numerical gradients:\n",
      "Difference for W1: tensor(4.7623, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Difference for b1: tensor(5.2553, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Difference for W2: tensor(2.7488, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Difference for b2: tensor(4.9268, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Relative error for W1: tensor(1.0000, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Relative error for b1: tensor(1.0000, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Relative error for W2: tensor(1.0000, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Relative error for b2: tensor(1.0000, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "\n",
      "Between torch and numerical gradients:\n",
      "Difference for W1: tensor(0.1238, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Difference for b1: tensor(0.1164, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Difference for W2: tensor(0.0891, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Difference for b2: tensor(0.1148, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Relative error for W1: tensor(1.0000, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Relative error for b1: tensor(1.0000, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Relative error for W2: tensor(1.0000, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Relative error for b2: tensor(1.0000, dtype=torch.float64, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "self.check_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
