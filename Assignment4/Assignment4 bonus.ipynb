{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Bonus - Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 80\n"
     ]
    }
   ],
   "source": [
    "book_fname = 'data/goblet_book.txt'\n",
    "with open(book_fname, 'r') as f:\n",
    "    book_data = f.read()\n",
    "    f.close()\n",
    "\n",
    "book_chars = list(set(book_data))\n",
    "K = len(book_chars)\n",
    "print(f\"Number of unique characters: {K}\")\n",
    "char2ind, ind2char = dict(), dict()\n",
    "for i, c in enumerate(book_chars):\n",
    "    char2ind[c] = i\n",
    "    ind2char[i] = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\n",
    "    def __init__(self, m=100, seq_length=25, eta=.001, gamma=.9, sig=.01, device=device):\n",
    "        self.m = m\n",
    "        self.seq_length = seq_length\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "        self.V = torch.randn(K, m).to(self.device) * sig\n",
    "        self.c = torch.zeros(K, 1).to(self.device)\n",
    "        self.W = torch.randn(m, m).to(self.device) * sig\n",
    "        self.b = torch.zeros(m, 1).to(self.device)\n",
    "        self.U = torch.randn(m, K).to(self.device) * sig\n",
    "\n",
    "        self.V_g_ada = torch.zeros(K, m).to(self.device)\n",
    "        self.c_g_ada = torch.zeros(K, 1).to(self.device)\n",
    "        self.W_g_ada = torch.zeros(m, m).to(self.device)\n",
    "        self.b_g_ada = torch.zeros(m, 1).to(self.device)\n",
    "        self.U_g_ada = torch.zeros(m, K).to(self.device)\n",
    "\n",
    "        self.V_v_ada = torch.zeros(K, m).to(self.device)\n",
    "        self.c_v_ada = torch.zeros(K, 1).to(self.device)\n",
    "        self.W_v_ada = torch.zeros(m, m).to(self.device)\n",
    "        self.b_v_ada = torch.zeros(m, 1).to(self.device)\n",
    "        self.U_v_ada = torch.zeros(m, K).to(self.device)\n",
    "        self.V_s_ada = torch.zeros(K, m).to(self.device)\n",
    "        self.c_s_ada = torch.zeros(K, 1).to(self.device)\n",
    "        self.W_s_ada = torch.zeros(m, m).to(self.device)\n",
    "        self.b_s_ada = torch.zeros(m, 1).to(self.device)\n",
    "        self.U_s_ada = torch.zeros(m, K).to(self.device)\n",
    "\n",
    "        self.V_best = self.V.clone()\n",
    "        self.c_best = self.c.clone()\n",
    "        self.W_best = self.W.clone()\n",
    "        self.b_best = self.b.clone()\n",
    "        self.U_best = self.U.clone()\n",
    "\n",
    "        self.smooth_losses = list()\n",
    "\n",
    "        self.grads = {\n",
    "            'V': torch.zeros_like(self.V),\n",
    "            'c': torch.zeros_like(self.c),\n",
    "            'W': torch.zeros_like(self.W),\n",
    "            'b': torch.zeros_like(self.b),\n",
    "            'U': torch.zeros_like(self.U)\n",
    "        }\n",
    "\n",
    "    def synthesize(self, h_prev, x, n, best=False):\n",
    "        Y = torch.zeros((K, n)).to(self.device)\n",
    "        x_t = x\n",
    "        for i in range(n):\n",
    "            h_prev, p = self.forward(h_prev, x_t, best=best)\n",
    "            cp = torch.cumsum(p, dim=0)\n",
    "            r = torch.rand(1)\n",
    "            for j in range(K):\n",
    "                if r < cp[j]:\n",
    "                    break\n",
    "            Y[j, i] = 1\n",
    "            x_t = torch.zeros((K, 1))\n",
    "            x_t[j] = 1\n",
    "        return Y\n",
    "\n",
    "    def forward(self, h_prev, x, best=False):\n",
    "        if not best:\n",
    "            h = torch.tanh(self.W @ h_prev + self.U @ x + self.b)\n",
    "            y = self.V @ h + self.c\n",
    "            p = torch.softmax(y, dim=0)\n",
    "        else:\n",
    "            h = torch.tanh(self.W_best @ h_prev + self.U_best @ x + self.b_best)\n",
    "            y = self.V_best @ h + self.c_best\n",
    "            p = torch.softmax(y, dim=0)\n",
    "        return h, p\n",
    "    \n",
    "    def forward_pass(self, h_0, X, Y, best=False):\n",
    "        h = h_0\n",
    "        H = torch.zeros((self.m, self.seq_length + 1)).to(self.device)\n",
    "        P = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "        H[:, 0] = h.flatten()\n",
    "        loss = 0\n",
    "        for i in range(self.seq_length):\n",
    "            h, p = self.forward(h, X[:, i].reshape(K, 1), best=best)\n",
    "            H[:, i+1] = h.flatten()\n",
    "            P[:, i] = p.flatten()\n",
    "            loss += self.loss(p, Y[:, i].reshape(K, 1))\n",
    "        return H, P, loss\n",
    "\n",
    "    def backward_pass(self, H, P, X, Y):\n",
    "        self.grads = {\n",
    "            'V': torch.zeros_like(self.V),\n",
    "            'c': torch.zeros_like(self.c),\n",
    "            'W': torch.zeros_like(self.W),\n",
    "            'b': torch.zeros_like(self.b),\n",
    "            'U': torch.zeros_like(self.U)\n",
    "        }\n",
    "        dL_dh_next = torch.zeros((self.m, 1)).to(self.device)\n",
    "        h0 = H[:, 0].reshape(self.m, 1)\n",
    "        H = H[:, 1:]\n",
    "        for i in range(self.seq_length-1, -1, -1):\n",
    "            x = X[:, i].reshape(K, 1)\n",
    "            y = Y[:, i].reshape(K, 1)\n",
    "            h = H[:, i].reshape(self.m, 1)\n",
    "            p = P[:, i].reshape(K, 1)\n",
    "            g = (p - y).T\n",
    "            self.grads['V'] += g.T @ h.T\n",
    "            self.grads['c'] += g.T\n",
    "            dL_dh = self.V.T @ g.T + self.W.T @ dL_dh_next\n",
    "            dL_dh_next = dL_dh * (1 - h ** 2)\n",
    "            self.grads['W'] += dL_dh_next @ H[:, i-1].reshape(1, self.m) if i != 0 else dL_dh_next @ h0.T\n",
    "            self.grads['b'] += dL_dh_next\n",
    "            self.grads['U'] += dL_dh_next @ x.T\n",
    "    \n",
    "    def update_params_adagrad(self, eps=1e-8):\n",
    "        for key in self.grads.keys():\n",
    "            self.grads[key] = torch.clamp(self.grads[key], -5, 5)\n",
    "            vars(self)[key + '_g_ada'] = self.gamma * vars(self)[key + '_g_ada'] + (1 - self.gamma) * self.grads[key] ** 2\n",
    "            vars(self)[key] -= self.eta * self.grads[key] / torch.sqrt(vars(self)[key + '_g_ada'] + eps)\n",
    "\n",
    "    def update_params_adam(self, t=1, beta1=.9, beta2=.999, eps=1e-8):\n",
    "        for key in self.grads.keys():\n",
    "            self.grads[key] = torch.clamp(self.grads[key], -5, 5)\n",
    "            vars(self)[key + '_v_ada'] = beta1 * vars(self)[key + '_v_ada'] + (1 - beta1) * self.grads[key]\n",
    "            vars(self)[key + '_s_ada'] = beta2 * vars(self)[key + '_s_ada'] + (1 - beta2) * self.grads[key] * self.grads[key]\n",
    "            v_hat = vars(self)[key + '_v_ada'] / (1 - beta1 ** t)\n",
    "            s_hat = vars(self)[key + '_s_ada'] / (1 - beta2 ** t)\n",
    "            vars(self)[key] -= self.eta * v_hat / (torch.sqrt(s_hat) + eps)\n",
    "\n",
    "    def loss(self, p, y):\n",
    "        return - torch.sum(y.T @ torch.log(p))\n",
    "\n",
    "    def train(self, book_data, n_epochs=10):\n",
    "        n_iter = 0\n",
    "        smooth_loss = 0\n",
    "        best_loss = torch.inf\n",
    "        pbar = trange(n_epochs)\n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch {epoch}/{n_epochs}\")\n",
    "            self.e = 0\n",
    "            h = torch.zeros((self.m, 1)).to(self.device)\n",
    "            while self.e + self.seq_length <= len(book_data):\n",
    "                n_iter += 1\n",
    "                X_chars = book_data[self.e:self.e+self.seq_length]\n",
    "                Y_chars = book_data[self.e+1:self.e+self.seq_length+1]\n",
    "                X = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "                Y = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "                for i in range(self.seq_length):\n",
    "                    X[char2ind[X_chars[i]], i] = 1\n",
    "                    Y[char2ind[Y_chars[i]], i] = 1\n",
    "\n",
    "                H, P, loss = self.forward_pass(h, X, Y)\n",
    "                self.backward_pass(H, P, X, Y)\n",
    "                # self.update_params_adagrad(eps)\n",
    "                self.update_params_adam(t=n_iter)\n",
    "\n",
    "                smooth_loss = .999 * smooth_loss + .001 * loss if smooth_loss != 0 else loss\n",
    "                self.smooth_losses.append(smooth_loss)\n",
    "\n",
    "                if smooth_loss < best_loss:\n",
    "                    best_loss = smooth_loss\n",
    "                    self.V_best = self.V.clone()\n",
    "                    self.c_best = self.c.clone()\n",
    "                    self.W_best = self.W.clone()\n",
    "                    self.b_best = self.b.clone()\n",
    "                    self.U_best = self.U.clone()\n",
    "\n",
    "                h = H[:, -1].reshape(self.m, 1)\n",
    "\n",
    "                if n_iter % 1000 == 0:\n",
    "                    print(\n",
    "                        f\"Training - Iteration {n_iter} - Loss {smooth_loss}\")\n",
    "                    \n",
    "                if n_iter % 10000 == 0:\n",
    "                    Y = self.synthesize(h, X[:, 0].reshape(K, 1), 200)\n",
    "                    print()\n",
    "                    print(''.join([ind2char[torch.argmax(Y[:, i]).item()] for i in range(200)]))\n",
    "                    print()\n",
    "\n",
    "                self.e += self.seq_length\n",
    "\n",
    "        print(f\"Training done - Best loss: {best_loss}\")\n",
    "\n",
    "    def check_grads(self, book_data):\n",
    "        for key in self.grads.keys():\n",
    "            vars(self)[key].requires_grad = True\n",
    "\n",
    "        X_chars = book_data[:self.seq_length]\n",
    "        Y_chars = book_data[1:self.seq_length+1]\n",
    "        X = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "        Y = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "        h0 = torch.zeros((self.m, 1)).to(self.device)\n",
    "        for i in range(self.seq_length):\n",
    "            X[char2ind[X_chars[i]], i] = 1\n",
    "            Y[char2ind[Y_chars[i]], i] = 1\n",
    "\n",
    "        H, P, loss = self.forward_pass(h0, X, Y)\n",
    "        self.backward_pass(H, P, X, Y)\n",
    "\n",
    "        for key in self.grads.keys():\n",
    "            vars(self)[key].retain_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        print(\"Checking gradients\")\n",
    "        with torch.no_grad():\n",
    "            for key in self.grads.keys():\n",
    "                diff = torch.norm(self.grads[key] - vars(self)[key].grad)\n",
    "                rel_err = diff / (torch.norm(self.grads[key]) + torch.norm(vars(self)[key].grad) + 1e-16)\n",
    "                print(f\"Relative error on {key}: {rel_err}\")\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.smooth_losses)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Smoothed loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradients\n",
      "Relative error on V: 2.7595367768640244e-08\n",
      "Relative error on c: 1.2321706321927195e-08\n",
      "Relative error on W: 3.842839291223754e-08\n",
      "Relative error on b: 3.8297052640245965e-08\n",
      "Relative error on U: 2.656783060217549e-08\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "rnn.check_grads(book_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3b0b31fdb440dd89df01a0260ccee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Iteration 1000 - Loss 86.9253921508789\n",
      "Training - Iteration 2000 - Loss 71.34590148925781\n",
      "Training - Iteration 3000 - Loss 62.55366897583008\n",
      "Training - Iteration 4000 - Loss 58.51759338378906\n",
      "Training - Iteration 5000 - Loss 55.91718673706055\n",
      "Training - Iteration 6000 - Loss 54.75102233886719\n",
      "Training - Iteration 7000 - Loss 54.15892028808594\n",
      "Training - Iteration 8000 - Loss 51.347286224365234\n",
      "Training - Iteration 9000 - Loss 50.22331237792969\n",
      "Training - Iteration 10000 - Loss 50.02109146118164\n",
      "\n",
      "\"\n",
      "\"I qubmellod Hormy yont swevar sonk at each if lo bor a douth wastponch said. . . Wat lave mowis very llann ta to hip lleted loows fo malied -'den, a stowley, an?\" stith yol it a bade dint the . Thi\n",
      "\n",
      "Training - Iteration 11000 - Loss 50.80976486206055\n",
      "Training - Iteration 12000 - Loss 50.15542984008789\n",
      "Training - Iteration 13000 - Loss 49.18244171142578\n",
      "Training - Iteration 14000 - Loss 48.52152633666992\n",
      "Training - Iteration 15000 - Loss 48.248172760009766\n",
      "Training - Iteration 16000 - Loss 47.19192123413086\n",
      "Training - Iteration 17000 - Loss 46.60337829589844\n",
      "Training - Iteration 18000 - Loss 46.5113410949707\n",
      "Training - Iteration 19000 - Loss 46.22223663330078\n",
      "Training - Iteration 20000 - Loss 45.44670867919922\n",
      "\n",
      "Ky ronen they could a reee wordoridagh a firs; nat, hich, he not got somptsing sown. . . Dimpreed.  I droked lookney, Rungared af weto Harry, I'm -\"\n",
      "\"Weve touss.\"\n",
      "\"I caach- fich said sawors reated qui\n",
      "\n",
      "Training - Iteration 21000 - Loss 44.9441032409668\n",
      "Training - Iteration 22000 - Loss 44.31604766845703\n",
      "Training - Iteration 23000 - Loss 44.544681549072266\n",
      "Training - Iteration 24000 - Loss 44.6644172668457\n",
      "Training - Iteration 25000 - Loss 44.78915023803711\n",
      "Training - Iteration 26000 - Loss 44.60860061645508\n",
      "Training - Iteration 27000 - Loss 44.3237419128418\n",
      "Training - Iteration 28000 - Loss 44.60897445678711\n",
      "Training - Iteration 29000 - Loss 43.546043395996094\n",
      "Training - Iteration 30000 - Loss 43.93607711791992\n",
      "\n",
      "likes.\n",
      "\"That's alle onl.  He ware mood soon.  Thie hous was gointed og the ngotyer saup an it wants\n",
      "Stanted lintly and lack.  Hegioaring theaning hever, Praching had been strnime.\n",
      "\"It sermovis to?  Ha\n",
      "\n",
      "Training - Iteration 31000 - Loss 43.926841735839844\n",
      "Training - Iteration 32000 - Loss 43.23785400390625\n",
      "Training - Iteration 33000 - Loss 43.138877868652344\n",
      "Training - Iteration 34000 - Loss 41.95958709716797\n",
      "Training - Iteration 35000 - Loss 42.08344268798828\n",
      "Training - Iteration 36000 - Loss 42.135318756103516\n",
      "Training - Iteration 37000 - Loss 42.315269470214844\n",
      "Training - Iteration 38000 - Loss 42.35908126831055\n",
      "Training - Iteration 39000 - Loss 41.87046813964844\n",
      "Training - Iteration 40000 - Loss 41.682437896728516\n",
      "\n",
      "han as whilaking the and it. Vood, table is - getsening to stunlith readonel slive of faciob if that is befort whits whitter croupl--cissing lift,\" said Voldemort - And folt treanis long of the ured f\n",
      "\n",
      "Training - Iteration 41000 - Loss 40.737674713134766\n",
      "Training - Iteration 42000 - Loss 40.39393997192383\n",
      "Training - Iteration 43000 - Loss 40.675262451171875\n",
      "Training - Iteration 44000 - Loss 40.844547271728516\n",
      "Training - Iteration 45000 - Loss 42.169288635253906\n",
      "Training - Iteration 46000 - Loss 42.550296783447266\n",
      "Training - Iteration 47000 - Loss 42.3118782043457\n",
      "Training - Iteration 48000 - Loss 41.951148986816406\n",
      "Training - Iteration 49000 - Loss 42.34369659423828\n",
      "Training - Iteration 50000 - Loss 43.09517288208008\n",
      "\n",
      "the watch settle on (Fred tail, Harry paambless steation agaishrers offes,\" He hands laking.  Seee fover's wezaverd to dofind beths Ik, some of, emeckife.\n",
      "\"The gut, he and loor, beleandy they?\"  thich\n",
      "\n",
      "Training - Iteration 51000 - Loss 43.959049224853516\n",
      "Training - Iteration 52000 - Loss 42.31596755981445\n",
      "Training - Iteration 53000 - Loss 40.65175247192383\n",
      "Training - Iteration 54000 - Loss 41.35700607299805\n",
      "Training - Iteration 55000 - Loss 43.02083969116211\n",
      "Training - Iteration 56000 - Loss 43.039241790771484\n",
      "Training - Iteration 57000 - Loss 42.5057487487793\n",
      "Training - Iteration 58000 - Loss 41.78406524658203\n",
      "Training - Iteration 59000 - Loss 42.52053451538086\n",
      "Training - Iteration 60000 - Loss 41.050575256347656\n",
      "\n",
      "nd as itred ter is slet. The Harry stowlit.\"\n",
      "\"Ier?\" . that's some.\n",
      "\"Hevill interemay. \"What say, And seat seeve, as a gnartingn the grous teed Snand to a that what see ind and Ron, there's some Magice\n",
      "\n",
      "Training - Iteration 61000 - Loss 40.58382034301758\n",
      "Training - Iteration 62000 - Loss 40.52102279663086\n",
      "Training - Iteration 63000 - Loss 40.99734878540039\n",
      "Training - Iteration 64000 - Loss 40.84749221801758\n",
      "Training - Iteration 65000 - Loss 40.39656066894531\n",
      "Training - Iteration 66000 - Loss 39.950584411621094\n",
      "Training - Iteration 67000 - Loss 40.80002975463867\n",
      "Training - Iteration 68000 - Loss 40.76695251464844\n",
      "Training - Iteration 69000 - Loss 41.073585510253906\n",
      "Training - Iteration 70000 - Loss 41.233402252197266\n",
      "\n",
      "icc; I wind they gives,\" Ron she's 'dd houred Burmy.  On the watchion moonged Magoseraused shark Snape!\" . .\"\n",
      "Vnit, he is as very pats. \"Noud Vinsted.  Lome the and a you he soteled.\n",
      "\"As steptary slap\n",
      "\n",
      "Training - Iteration 71000 - Loss 41.32613754272461\n",
      "Training - Iteration 72000 - Loss 41.4026985168457\n",
      "Training - Iteration 73000 - Loss 40.59427261352539\n",
      "Training - Iteration 74000 - Loss 40.76044464111328\n",
      "Training - Iteration 75000 - Loss 40.28942108154297\n",
      "Training - Iteration 76000 - Loss 40.34254455566406\n",
      "Training - Iteration 77000 - Loss 39.99661636352539\n",
      "Training - Iteration 78000 - Loss 39.64372253417969\n",
      "Training - Iteration 79000 - Loss 38.775352478027344\n",
      "Training - Iteration 80000 - Loss 39.42232894897461\n",
      "\n",
      "ions, shosed.\n",
      "\"With stoptlonger, whil . . .\"\n",
      "\"Yes upragent see with e tere to yeeply.  He was, if the dare heart was aroun quiticily I why to to the dikce hatter. . . .h. . . . repultianding it it. Du\n",
      "\n",
      "Training - Iteration 81000 - Loss 38.731929779052734\n",
      "Training - Iteration 82000 - Loss 39.436370849609375\n",
      "Training - Iteration 83000 - Loss 39.22632598876953\n",
      "Training - Iteration 84000 - Loss 39.97624206542969\n",
      "Training - Iteration 85000 - Loss 38.3076057434082\n",
      "Training - Iteration 86000 - Loss 38.26917266845703\n",
      "Training - Iteration 87000 - Loss 38.32228088378906\n",
      "Training - Iteration 88000 - Loss 38.49656295776367\n",
      "Training - Iteration 89000 - Loss 39.85211944580078\n",
      "Training - Iteration 90000 - Loss 39.78200912475586\n",
      "\n",
      "amoded,\n",
      "Harre snallisk from a switched his peosed molf bet wanth Harry in hmore without of ane he was at his Diptowed that hard kidley shorter'll at the beraite stand with he had been who him for tris\n",
      "\n",
      "Training - Iteration 91000 - Loss 40.48973083496094\n",
      "Training - Iteration 92000 - Loss 39.88190460205078\n",
      "Training - Iteration 93000 - Loss 40.44968795776367\n",
      "Training - Iteration 94000 - Loss 41.33671951293945\n",
      "Training - Iteration 95000 - Loss 42.313072204589844\n",
      "Training - Iteration 96000 - Loss 41.250850677490234\n",
      "Training - Iteration 97000 - Loss 39.58669662475586\n",
      "Training - Iteration 98000 - Loss 39.93108367919922\n",
      "Training - Iteration 99000 - Loss 40.74843215942383\n",
      "Training - Iteration 100000 - Loss 41.02384567260742\n",
      "\n",
      " thit was stupbed one it, it - mut the Amminded be allorm abound up abrixely a thr were again.\n",
      "\"Ah sevioned as the know,\" said Ron, seen, dieker on it them. .\n",
      "\"Boreast, and as nevith, alough he doorse\n",
      "\n",
      "Training - Iteration 101000 - Loss 41.030818939208984\n",
      "Training - Iteration 102000 - Loss 40.12995910644531\n",
      "Training - Iteration 103000 - Loss 41.0418815612793\n",
      "Training - Iteration 104000 - Loss 39.82912063598633\n",
      "Training - Iteration 105000 - Loss 38.82122802734375\n",
      "Training - Iteration 106000 - Loss 38.67682647705078\n",
      "Training - Iteration 107000 - Loss 39.39822769165039\n",
      "Training - Iteration 108000 - Loss 39.23992919921875\n",
      "Training - Iteration 109000 - Loss 39.091651916503906\n",
      "Training - Iteration 110000 - Loss 38.932613372802734\n",
      "\n",
      "Harry watched it on a but me. Fine didn't stince he'd judge from the ecre-the dragons onting Mcionite - Quirtte it it was a ot....\n",
      "Fut became blinkars and the very set. . . Harry, he thing on thim bef\n",
      "\n",
      "Training - Iteration 111000 - Loss 39.46289825439453\n",
      "Training - Iteration 112000 - Loss 39.15443801879883\n",
      "Training - Iteration 113000 - Loss 39.49198532104492\n",
      "Training - Iteration 114000 - Loss 39.992774963378906\n",
      "Training - Iteration 115000 - Loss 40.35335159301758\n",
      "Training - Iteration 116000 - Loss 39.48147201538086\n",
      "Training - Iteration 117000 - Loss 40.318702697753906\n",
      "Training - Iteration 118000 - Loss 40.0331916809082\n",
      "Training - Iteration 119000 - Loss 39.111228942871094\n",
      "Training - Iteration 120000 - Loss 39.468849182128906\n",
      "\n",
      "red how mostat - listleed down again, now sniwark sont loom usquening the undred to maker to the hopirn. \n",
      "\"A celdts folt them tommess fobrity scaded it.\n",
      "Pother and was bog, please blaught of the compo\n",
      "\n",
      "Training - Iteration 121000 - Loss 38.67972946166992\n",
      "Training - Iteration 122000 - Loss 39.28330612182617\n",
      "Training - Iteration 123000 - Loss 37.4238166809082\n",
      "Training - Iteration 124000 - Loss 38.0538330078125\n",
      "Training - Iteration 125000 - Loss 37.988922119140625\n",
      "Training - Iteration 126000 - Loss 38.22209167480469\n",
      "Training - Iteration 127000 - Loss 38.50691604614258\n",
      "Training - Iteration 128000 - Loss 39.159950256347656\n",
      "Training - Iteration 129000 - Loss 37.76272964477539\n",
      "Training - Iteration 130000 - Loss 37.85331726074219\n",
      "\n",
      "happelon to is armorten oned andind and peritain might been me with his moubly body on courss ever with to Sirsupy me into sor..  I was stublitimate.  Who wiRkand the hop acountice more to said Demat \n",
      "\n",
      "Training - Iteration 131000 - Loss 36.96693801879883\n",
      "Training - Iteration 132000 - Loss 37.587825775146484\n",
      "Training - Iteration 133000 - Loss 38.639427185058594\n",
      "Training - Iteration 134000 - Loss 39.12599182128906\n",
      "Training - Iteration 135000 - Loss 39.460289001464844\n",
      "Training - Iteration 136000 - Loss 39.057491302490234\n",
      "Training - Iteration 137000 - Loss 39.545249938964844\n",
      "Training - Iteration 138000 - Loss 40.15367889404297\n",
      "Training - Iteration 139000 - Loss 40.7432746887207\n",
      "Training - Iteration 140000 - Loss 41.291534423828125\n",
      "\n",
      "box you did throw he capperedn trair shuneed on the and clase and dlasten a pashan haw seen pointing in the gave their rain cudgenal sent the plair fattered the vaster everyla casplan,\" Mud tebming, \"\n",
      "\n",
      "Training - Iteration 141000 - Loss 39.36174011230469\n",
      "Training - Iteration 142000 - Loss 38.90945816040039\n",
      "Training - Iteration 143000 - Loss 39.688568115234375\n",
      "Training - Iteration 144000 - Loss 40.77900314331055\n",
      "Training - Iteration 145000 - Loss 40.86086654663086\n",
      "Training - Iteration 146000 - Loss 39.755592346191406\n",
      "Training - Iteration 147000 - Loss 40.380279541015625\n",
      "Training - Iteration 148000 - Loss 39.6951789855957\n",
      "Training - Iteration 149000 - Loss 38.59825897216797\n",
      "Training - Iteration 150000 - Loss 38.199249267578125\n",
      "\n",
      "conered about he being to pattered't really at as Dumby,\" said down and give going and sure had roith liplaibls; he subber when handing and say it cabliggered in an have thuater, all reslee's -'ziggan\n",
      "\n",
      "Training - Iteration 151000 - Loss 38.51228713989258\n",
      "Training - Iteration 152000 - Loss 38.98871994018555\n",
      "Training - Iteration 153000 - Loss 38.36589813232422\n",
      "Training - Iteration 154000 - Loss 38.31266403198242\n",
      "Training - Iteration 155000 - Loss 38.2368278503418\n",
      "Training - Iteration 156000 - Loss 38.36321258544922\n",
      "Training - Iteration 157000 - Loss 38.52151870727539\n",
      "Training - Iteration 158000 - Loss 39.22429275512695\n",
      "Training - Iteration 159000 - Loss 39.31432342529297\n",
      "Training - Iteration 160000 - Loss 39.10343933105469\n",
      "\n",
      "prowe.  She sound with the dabled interest the likt about instural saying that wherus like year scarmicily we?\"\n",
      "\"Now that's top. the lang dit of a pointed with a.  \"I wouldn's got peiple panients.  Ha\n",
      "\n",
      "Training - Iteration 161000 - Loss 39.93903732299805\n",
      "Training - Iteration 162000 - Loss 38.8828239440918\n",
      "Training - Iteration 163000 - Loss 39.23615646362305\n",
      "Training - Iteration 164000 - Loss 39.16866683959961\n",
      "Training - Iteration 165000 - Loss 38.50199890136719\n",
      "Training - Iteration 166000 - Loss 38.91653060913086\n",
      "Training - Iteration 167000 - Loss 37.34409713745117\n",
      "Training - Iteration 168000 - Loss 37.79617691040039\n",
      "Training - Iteration 169000 - Loss 37.83091354370117\n",
      "Training - Iteration 170000 - Loss 38.0927848815918\n",
      "\n",
      "ped to his from the sobath from the bech the Dark Nane to the boreed out howlfutione shake the vilated, he bagminger Harry fall of his filly asain to givily a moster,\" Ron, he did of them dowgitat set\n",
      "\n",
      "Training - Iteration 171000 - Loss 38.381072998046875\n",
      "Training - Iteration 172000 - Loss 38.18002700805664\n",
      "Training - Iteration 173000 - Loss 37.74575424194336\n",
      "Training - Iteration 174000 - Loss 37.228797912597656\n",
      "Training - Iteration 175000 - Loss 36.404354095458984\n",
      "Training - Iteration 176000 - Loss 37.031925201416016\n",
      "Training - Iteration 177000 - Loss 37.34712600708008\n",
      "Training - Iteration 178000 - Loss 38.97860336303711\n",
      "Training - Iteration 179000 - Loss 39.269893646240234\n",
      "Training - Iteration 180000 - Loss 38.81372833251953\n",
      "\n",
      "\"Harry.\n",
      "Well, and (uncappied, the matmosinnly tut trust on his oordet bleasing my that of right as all they had going hocking to get head.\n",
      "\"Dunficiat thand to the of his exthertuad aggeal.  Harry fest\n",
      "\n",
      "Training - Iteration 181000 - Loss 39.06108856201172\n",
      "Training - Iteration 182000 - Loss 39.50897216796875\n",
      "Training - Iteration 183000 - Loss 40.56025314331055\n",
      "Training - Iteration 184000 - Loss 41.1951789855957\n",
      "Training - Iteration 185000 - Loss 39.50495529174805\n",
      "Training - Iteration 186000 - Loss 38.21307373046875\n",
      "Training - Iteration 187000 - Loss 39.07014465332031\n",
      "Training - Iteration 188000 - Loss 40.38127136230469\n",
      "Training - Iteration 189000 - Loss 40.506797790527344\n",
      "Training - Iteration 190000 - Loss 39.72881317138672\n",
      "\n",
      " a that back of the,\" said Moody cauld Cry started again. Mashe calter Sperted.\n",
      "\"Ip up accestly, and \"his beckufining mosting cloddered rirsary and mut heads couging Curse, skulls comithing stinn.\n",
      "\"No\n",
      "\n",
      "Training - Iteration 191000 - Loss 39.2329216003418\n",
      "Training - Iteration 192000 - Loss 39.80259323120117\n",
      "Training - Iteration 193000 - Loss 38.34023666381836\n",
      "Training - Iteration 194000 - Loss 37.92935562133789\n",
      "Training - Iteration 195000 - Loss 37.9752197265625\n",
      "Training - Iteration 196000 - Loss 38.559539794921875\n",
      "Training - Iteration 197000 - Loss 38.58497619628906\n",
      "Training - Iteration 198000 - Loss 38.11761474609375\n",
      "Training - Iteration 199000 - Loss 37.55425262451172\n",
      "Training - Iteration 200000 - Loss 38.423980712890625\n",
      "\n",
      " haveng Dobic has someth!\"  said Harry arm;ated un the Horm bartiog down, effin furoll  Dobf withal, him.  It's bit that armored in they holdibllowed, shocked, PonFid Disualy down trick around all to \n",
      "\n",
      "Training - Iteration 201000 - Loss 38.63459396362305\n",
      "Training - Iteration 202000 - Loss 38.808692932128906\n",
      "Training - Iteration 203000 - Loss 39.18328857421875\n",
      "Training - Iteration 204000 - Loss 38.94898223876953\n",
      "Training - Iteration 205000 - Loss 39.458946228027344\n",
      "Training - Iteration 206000 - Loss 38.43214797973633\n",
      "Training - Iteration 207000 - Loss 38.70358657836914\n",
      "Training - Iteration 208000 - Loss 38.55141830444336\n",
      "Training - Iteration 209000 - Loss 38.27316665649414\n",
      "Training - Iteration 210000 - Loss 38.30855178833008\n",
      "\n",
      " somed low long wizarding to her  And the hometh wery page were emitting Medrogg of he seer Flet chirtion.\n",
      "\"She'd ewardge plady that patureation.  The cushital ploppifer!\"\n",
      "\"Bock to the gots at Harry. \n",
      "\n",
      "Training - Iteration 211000 - Loss 37.23390579223633\n",
      "Training - Iteration 212000 - Loss 37.18703842163086\n",
      "Training - Iteration 213000 - Loss 37.466888427734375\n",
      "Training - Iteration 214000 - Loss 36.94322967529297\n",
      "Training - Iteration 215000 - Loss 37.64944839477539\n",
      "Training - Iteration 216000 - Loss 37.467159271240234\n",
      "Training - Iteration 217000 - Loss 37.90900421142578\n",
      "Training - Iteration 218000 - Loss 36.77622604370117\n",
      "Training - Iteration 219000 - Loss 36.4050407409668\n",
      "Training - Iteration 220000 - Loss 36.63939666748047\n",
      "\n",
      "t - over the drepped to it, Voldemort, again, you saw the facis, I don's dazed like you - quirasalling of the secte him stopped evos sleep acrep, and Voldemort giveding youarin told as that Dempioning\n",
      "\n",
      "Training - Iteration 221000 - Loss 37.013336181640625\n",
      "Training - Iteration 222000 - Loss 38.4632453918457\n",
      "Training - Iteration 223000 - Loss 38.1653938293457\n",
      "Training - Iteration 224000 - Loss 38.808677673339844\n",
      "Training - Iteration 225000 - Loss 38.3612060546875\n",
      "Training - Iteration 226000 - Loss 39.09065246582031\n",
      "Training - Iteration 227000 - Loss 39.960689544677734\n",
      "Training - Iteration 228000 - Loss 41.0252571105957\n",
      "Training - Iteration 229000 - Loss 39.62605667114258\n",
      "Training - Iteration 230000 - Loss 37.81122589111328\n",
      "\n",
      "ng!\"\n",
      "Winky eye. ,\"Ears, I won't mosing aim belling Ghor!\"\n",
      "\"I aps.  Ho sery.  Ex.\"\n",
      "\"Do Iristoneding his;'s wanted it os-'y ever her siwn deasleys into.  We keezled nomes.  \"Bush, ared flush's it'll at \n",
      "\n",
      "Training - Iteration 231000 - Loss 38.542747497558594\n",
      "Training - Iteration 232000 - Loss 39.1292839050293\n",
      "Training - Iteration 233000 - Loss 39.70082092285156\n",
      "Training - Iteration 234000 - Loss 39.521095275878906\n",
      "Training - Iteration 235000 - Loss 38.88682556152344\n",
      "Training - Iteration 236000 - Loss 39.50929260253906\n",
      "Training - Iteration 237000 - Loss 37.981353759765625\n",
      "Training - Iteration 238000 - Loss 37.58259582519531\n",
      "Training - Iteration 239000 - Loss 37.166954040527344\n",
      "Training - Iteration 240000 - Loss 37.968544006347656\n",
      "\n",
      "think to he coulder.  Harry.\n",
      "\"Harry!\"  said Hermione of uronly of but they quause, to be as about the Quidgep!\"  Huncing relest had him at alforment, he's 'ywarded him had for might.\n",
      "\"Of that got a we\n",
      "\n",
      "Training - Iteration 241000 - Loss 37.784976959228516\n",
      "Training - Iteration 242000 - Loss 37.6536979675293\n",
      "Training - Iteration 243000 - Loss 37.70764923095703\n",
      "Training - Iteration 244000 - Loss 38.47932434082031\n",
      "Training - Iteration 245000 - Loss 37.88302993774414\n",
      "Training - Iteration 246000 - Loss 38.18403625488281\n",
      "Training - Iteration 247000 - Loss 38.5265007019043\n",
      "Training - Iteration 248000 - Loss 39.47701644897461\n",
      "Training - Iteration 249000 - Loss 38.638648986816406\n",
      "Training - Iteration 250000 - Loss 39.026649475097656\n",
      "\n",
      "tair and didgried aroment of Moodace earsing saw his in of Wipples, and stresie pulder.\n",
      "\"Snowerteps.  \"Buttily, and there.  \"It wave mador.  But Snape stapt, pock unk Harry hanther tone his coppoyest \n",
      "\n",
      "Training - Iteration 251000 - Loss 38.79883575439453\n",
      "Training - Iteration 252000 - Loss 38.0063591003418\n",
      "Training - Iteration 253000 - Loss 38.45570755004883\n",
      "Training - Iteration 254000 - Loss 37.6190071105957\n",
      "Training - Iteration 255000 - Loss 37.95998001098633\n",
      "Training - Iteration 256000 - Loss 36.46025466918945\n",
      "Training - Iteration 257000 - Loss 36.9843864440918\n",
      "Training - Iteration 258000 - Loss 36.55192947387695\n",
      "Training - Iteration 259000 - Loss 37.09307861328125\n",
      "Training - Iteration 260000 - Loss 37.24160385131836\n",
      "\n",
      "l sigry. Harry corny.  And saw you anothing it points and Crothed walt not, whoirs cold bairt his largls.  A think have buck him.\n",
      "She wagment snitwon, he\" it memponse.\n",
      "Apolsering enchone studen porth \n",
      "\n",
      "Training - Iteration 261000 - Loss 37.89070510864258\n",
      "Training - Iteration 262000 - Loss 36.45793533325195\n",
      "Training - Iteration 263000 - Loss 36.58889389038086\n",
      "Training - Iteration 264000 - Loss 35.97648239135742\n",
      "Training - Iteration 265000 - Loss 36.30715560913086\n",
      "Training - Iteration 266000 - Loss 37.92630386352539\n",
      "Training - Iteration 267000 - Loss 38.00062561035156\n",
      "Training - Iteration 268000 - Loss 38.386329650878906\n",
      "Training - Iteration 269000 - Loss 37.96326446533203\n",
      "Training - Iteration 270000 - Loss 38.516563415527344\n",
      "\n",
      "on the moveate to sumber was socked thing flum of Gryffind them pared.  \"How curlen been tongapetere turs't each tertaging intains wizards around the incession into Mrs. Weasley watchure after to fini\n",
      "\n",
      "Training - Iteration 271000 - Loss 39.45783996582031\n",
      "Training - Iteration 272000 - Loss 39.99528884887695\n",
      "Training - Iteration 273000 - Loss 40.27302932739258\n",
      "Training - Iteration 274000 - Loss 38.2642822265625\n",
      "Training - Iteration 275000 - Loss 38.354469299316406\n",
      "Training - Iteration 276000 - Loss 38.75516128540039\n",
      "Training - Iteration 277000 - Loss 39.707035064697266\n",
      "Training - Iteration 278000 - Loss 39.98070526123047\n",
      "Training - Iteration 279000 - Loss 38.442054748535156\n",
      "Training - Iteration 280000 - Loss 39.311012268066406\n",
      "\n",
      "ed thear else to grotters. Thoy starty.\n",
      "\"I'll the Peath Eaply anying of the rels voice by a grifts's to behind the Granmed,\" said Gryffindor a from the parin, both's could year Gang the seachmet. . . \n",
      "\n",
      "Training - Iteration 281000 - Loss 38.386390686035156\n",
      "Training - Iteration 282000 - Loss 37.632083892822266\n",
      "Training - Iteration 283000 - Loss 37.08280563354492\n",
      "Training - Iteration 284000 - Loss 37.68879318237305\n",
      "Training - Iteration 285000 - Loss 37.95607376098633\n",
      "Training - Iteration 286000 - Loss 37.27631378173828\n",
      "Training - Iteration 287000 - Loss 37.3813362121582\n",
      "Training - Iteration 288000 - Loss 37.66268539428711\n",
      "Training - Iteration 289000 - Loss 37.60160827636719\n",
      "Training - Iteration 290000 - Loss 37.57798385620117\n",
      "\n",
      "Madam.\n",
      "\"I'm Asters, \"plit.\"\n",
      "Harry some.  Ron tither, . . .\"\n",
      "\"I  Nevell...\n",
      "Ron looking me, were day. \"She id to the was not studiche,\" hers inside Harry realife's now,\" said Ginny  Freeveesting to heak\n",
      "\n",
      "Training - Iteration 291000 - Loss 38.7086296081543\n",
      "Training - Iteration 292000 - Loss 38.226680755615234\n",
      "Training - Iteration 293000 - Loss 38.20549011230469\n",
      "Training - Iteration 294000 - Loss 39.024383544921875\n",
      "Training - Iteration 295000 - Loss 38.12090301513672\n",
      "Training - Iteration 296000 - Loss 38.279441833496094\n",
      "Training - Iteration 297000 - Loss 38.362144470214844\n",
      "Training - Iteration 298000 - Loss 37.4976692199707\n",
      "Training - Iteration 299000 - Loss 38.0277099609375\n",
      "Training - Iteration 300000 - Loss 36.54132843017578\n",
      "\n",
      "wed as Fred.  \"WeHt could he would Percy's fore to it's no hamen over can, and a cleas he neming to into  \"What wlat by a gone the humsing Hermione, Fred,\" he said him and Fred stup mina's work,\" Harr\n",
      "\n",
      "Training - Iteration 301000 - Loss 37.12699508666992\n",
      "Training - Iteration 302000 - Loss 36.965065002441406\n",
      "Training - Iteration 303000 - Loss 37.251129150390625\n",
      "Training - Iteration 304000 - Loss 37.4072151184082\n",
      "Training - Iteration 305000 - Loss 37.44351577758789\n",
      "Training - Iteration 306000 - Loss 36.705223083496094\n",
      "Training - Iteration 307000 - Loss 36.28617477416992\n",
      "Training - Iteration 308000 - Loss 35.516143798828125\n",
      "Training - Iteration 309000 - Loss 36.00835418701172\n",
      "Training - Iteration 310000 - Loss 36.643314361572266\n",
      "\n",
      "ld have might sajua Karkyor he's the adimoned.\n",
      "\"Sobs once at Kar!\"\n",
      "\"The way then go much the Lorry wizard as you Potter?\" Siven't going he walk -\" ;ams with he filled it all his roble wouldn't yet ont\n",
      "\n",
      "Training - Iteration 311000 - Loss 37.86890411376953\n",
      "Training - Iteration 312000 - Loss 38.55645751953125\n",
      "Training - Iteration 313000 - Loss 37.88667297363281\n",
      "Training - Iteration 314000 - Loss 38.37156295776367\n",
      "Training - Iteration 315000 - Loss 39.09413528442383\n",
      "Training - Iteration 316000 - Loss 39.73657989501953\n",
      "Training - Iteration 317000 - Loss 40.42670440673828\n",
      "Training - Iteration 318000 - Loss 38.46612548828125\n",
      "Training - Iteration 319000 - Loss 37.62913513183594\n",
      "Training - Iteration 320000 - Loss 38.028926849365234\n",
      "\n",
      "y, whisped to do tine them.  It's -\"\n",
      "When he?\"  Billw.\n",
      "\"Yeah, Unstared Hira!\"  R What goide and Us into his greammoned mum' of through rubsible,\" said Fredger to gen than the back sid the Rishe, Hermi\n",
      "\n",
      "Training - Iteration 321000 - Loss 39.600711822509766\n",
      "Training - Iteration 322000 - Loss 39.76435089111328\n",
      "Training - Iteration 323000 - Loss 38.79134750366211\n",
      "Training - Iteration 324000 - Loss 38.46424102783203\n",
      "Training - Iteration 325000 - Loss 38.6892204284668\n",
      "Training - Iteration 326000 - Loss 37.7662239074707\n",
      "Training - Iteration 327000 - Loss 37.106895446777344\n",
      "Training - Iteration 328000 - Loss 37.31581115722656\n",
      "Training - Iteration 329000 - Loss 37.785343170166016\n",
      "Training - Iteration 330000 - Loss 37.623538970947266\n",
      "\n",
      "for the faren from the Mine stoors, readow downden the Cray. . . . Mad Coddce and keepeling. . . nothing to be funsitely would.\n",
      "The him, was dreps past that Harry seemes. . . . Peppor task pulled that\n",
      "\n",
      "Training - Iteration 331000 - Loss 37.28247833251953\n",
      "Training - Iteration 332000 - Loss 36.80121994018555\n",
      "Training - Iteration 333000 - Loss 37.58570861816406\n",
      "Training - Iteration 334000 - Loss 38.028743743896484\n",
      "Training - Iteration 335000 - Loss 38.056461334228516\n",
      "Training - Iteration 336000 - Loss 38.37674331665039\n",
      "Training - Iteration 337000 - Loss 38.13264465332031\n",
      "Training - Iteration 338000 - Loss 38.79459762573242\n",
      "Training - Iteration 339000 - Loss 37.59846878051758\n",
      "Training - Iteration 340000 - Loss 38.12873077392578\n",
      "\n",
      "ight of the face that hand old to folden into the look, and erican pall weed of dis despercowly, standenly out; geemed.  The back all was weath somects walling of the giegally flass surms and Gono. . \n",
      "\n",
      "Training - Iteration 341000 - Loss 38.32371139526367\n",
      "Training - Iteration 342000 - Loss 37.5493049621582\n",
      "Training - Iteration 343000 - Loss 37.813724517822266\n",
      "Training - Iteration 344000 - Loss 36.55451965332031\n",
      "Training - Iteration 345000 - Loss 36.470001220703125\n",
      "Training - Iteration 346000 - Loss 36.76874542236328\n",
      "Training - Iteration 347000 - Loss 36.59900665283203\n",
      "Training - Iteration 348000 - Loss 36.815826416015625\n",
      "Training - Iteration 349000 - Loss 36.839866638183594\n",
      "Training - Iteration 350000 - Loss 37.127506256103516\n",
      "\n",
      "n anything in the were pain found this sugges. ..\" I deys lieged downly, he had neally lit second facing, roberly, was a neature been mind inssent - was anize. - a forned slice, and dilling a no try'f\n",
      "\n",
      "Training - Iteration 351000 - Loss 36.14358139038086\n",
      "Training - Iteration 352000 - Loss 35.655364990234375\n",
      "Training - Iteration 353000 - Loss 35.9786491394043\n",
      "Training - Iteration 354000 - Loss 36.1811637878418\n",
      "Training - Iteration 355000 - Loss 37.84625244140625\n",
      "Training - Iteration 356000 - Loss 37.838714599609375\n",
      "Training - Iteration 357000 - Loss 38.32709503173828\n",
      "Training - Iteration 358000 - Loss 37.88786697387695\n",
      "Training - Iteration 359000 - Loss 38.46931838989258\n",
      "Training - Iteration 360000 - Loss 39.60935974121094\n",
      "\n",
      "ooked.  \"Tears itsered cor thellead of minoughbods know was alausiaKly, I dyel sign reallwant, pulling at a his on I could my lears, a mut barments facirg very slighting and facilf.\n",
      "\"Yeavy, in flause \n",
      "\n",
      "Training - Iteration 361000 - Loss 40.386199951171875\n",
      "Training - Iteration 362000 - Loss 39.01420211791992\n",
      "Training - Iteration 363000 - Loss 37.031497955322266\n",
      "Training - Iteration 364000 - Loss 37.92799758911133\n",
      "Training - Iteration 365000 - Loss 38.90501403808594\n",
      "Training - Iteration 366000 - Loss 39.194759368896484\n",
      "Training - Iteration 367000 - Loss 39.01884841918945\n",
      "Training - Iteration 368000 - Loss 38.2921142578125\n",
      "Training - Iteration 369000 - Loss 39.042903900146484\n",
      "Training - Iteration 370000 - Loss 37.34331512451172\n",
      "\n",
      "Gonation, lobrang a turned bust prop iy forgo a me you seemed out few Fred and Gryffiedone, Own Beop Fules, personso down George, arome did said, ham. Iven were for the a anling Fled. \"I've don, the G\n",
      "\n",
      "Training - Iteration 371000 - Loss 36.84944534301758\n",
      "Training - Iteration 372000 - Loss 36.76060485839844\n",
      "Training - Iteration 373000 - Loss 37.61743927001953\n",
      "Training - Iteration 374000 - Loss 37.44697189331055\n",
      "Training - Iteration 375000 - Loss 37.0828742980957\n",
      "Training - Iteration 376000 - Loss 36.86269760131836\n",
      "Training - Iteration 377000 - Loss 37.69619369506836\n",
      "Training - Iteration 378000 - Loss 37.58454132080078\n",
      "Training - Iteration 379000 - Loss 37.710872650146484\n",
      "Training - Iteration 380000 - Loss 38.043514251708984\n",
      "\n",
      "ly added that tway touy look and said with,\" cazing the Wizkenching? \"My you think more - What you know!\" he's here no lettle off her mistuld sides severidage inside through the pictle, two dred thim \n",
      "\n",
      "Training - Iteration 381000 - Loss 38.61696243286133\n",
      "Training - Iteration 382000 - Loss 38.35836410522461\n",
      "Training - Iteration 383000 - Loss 38.19859313964844\n",
      "Training - Iteration 384000 - Loss 38.10016632080078\n",
      "Training - Iteration 385000 - Loss 37.54384231567383\n",
      "Training - Iteration 386000 - Loss 37.93604278564453\n",
      "Training - Iteration 387000 - Loss 37.003395080566406\n",
      "Training - Iteration 388000 - Loss 37.234439849853516\n",
      "Training - Iteration 389000 - Loss 35.872310638427734\n",
      "Training - Iteration 390000 - Loss 36.637840270996094\n",
      "\n",
      "ark.  We woul on the gavely, his head of the worn - you knet in the worked.  \"The pastioned momezle of. Moody.\n",
      "Cark Dumbledore caring to ticed to be care. ay the bart to man him. Lout legsing, awound \n",
      "\n",
      "Training - Iteration 391000 - Loss 35.82807922363281\n",
      "Training - Iteration 392000 - Loss 36.53803634643555\n",
      "Training - Iteration 393000 - Loss 36.48185729980469\n",
      "Training - Iteration 394000 - Loss 37.659427642822266\n",
      "Training - Iteration 395000 - Loss 35.936729431152344\n",
      "Training - Iteration 396000 - Loss 35.87152099609375\n",
      "Training - Iteration 397000 - Loss 35.37492752075195\n",
      "Training - Iteration 398000 - Loss 35.82013702392578\n",
      "Training - Iteration 399000 - Loss 37.614078521728516\n",
      "Training - Iteration 400000 - Loss 37.401031494140625\n",
      "\n",
      "onaged to the something.  Megucly was any it, was no theit him byeah with And it had studer acrid Villy But that was on his beally at Hogwarts were his backling time in the only both after nother how \n",
      "\n",
      "Training - Iteration 401000 - Loss 38.074214935302734\n",
      "Training - Iteration 402000 - Loss 37.57281494140625\n",
      "Training - Iteration 403000 - Loss 38.17726516723633\n",
      "Training - Iteration 404000 - Loss 39.140708923339844\n",
      "Training - Iteration 405000 - Loss 39.59140396118164\n",
      "Training - Iteration 406000 - Loss 39.253814697265625\n",
      "Training - Iteration 407000 - Loss 37.60440444946289\n",
      "Training - Iteration 408000 - Loss 37.678829193115234\n",
      "Training - Iteration 409000 - Loss 38.394500732421875\n",
      "Training - Iteration 410000 - Loss 39.178890228271484\n",
      "\n",
      "leging as thore and gasilfere sevelus tco unnige qupence.  It wosstiply as its which of mighter that worked when Harry wilt at the givety firiffetly on for.  Harry go wouldn't moke that will on a comp\n",
      "\n",
      "Training - Iteration 411000 - Loss 39.397132873535156\n",
      "Training - Iteration 412000 - Loss 38.1191291809082\n",
      "Training - Iteration 413000 - Loss 39.06733703613281\n",
      "Training - Iteration 414000 - Loss 37.825347900390625\n",
      "Training - Iteration 415000 - Loss 36.8357048034668\n",
      "Training - Iteration 416000 - Loss 36.62217330932617\n",
      "Training - Iteration 417000 - Loss 37.40201187133789\n",
      "Training - Iteration 418000 - Loss 37.41988754272461\n",
      "Training - Iteration 419000 - Loss 36.91597366333008\n",
      "Training - Iteration 420000 - Loss 36.930015563964844\n",
      "\n",
      "rients hagnifing nermle, mertmess - more less had could hand don't heard ever opence, and stilas' friends to gredone chanchent and wongess.\n",
      "Ansple long were a gut he had hear bares each up.  Cedric wa\n",
      "\n",
      "Training - Iteration 421000 - Loss 37.37579345703125\n",
      "Training - Iteration 422000 - Loss 37.177791595458984\n",
      "Training - Iteration 423000 - Loss 37.334537506103516\n",
      "Training - Iteration 424000 - Loss 38.287235260009766\n",
      "Training - Iteration 425000 - Loss 37.75393295288086\n",
      "Training - Iteration 426000 - Loss 37.57620620727539\n",
      "Training - Iteration 427000 - Loss 38.588600158691406\n",
      "Training - Iteration 428000 - Loss 37.81679153442383\n",
      "Training - Iteration 429000 - Loss 37.5836181640625\n",
      "Training - Iteration 430000 - Loss 37.716270446777344\n",
      "\n",
      "he Inveriden he couldn', and Ron and Snape,\n",
      "Hurmilifing up the could your in they custle of the didn's als Know you cartain sked soffous sconethen Snape's bed eyester. . .\"   Harry, is posere worn Sni\n",
      "\n",
      "Training - Iteration 431000 - Loss 36.95780944824219\n",
      "Training - Iteration 432000 - Loss 37.577152252197266\n",
      "Training - Iteration 433000 - Loss 35.903778076171875\n",
      "Training - Iteration 434000 - Loss 36.4769287109375\n",
      "Training - Iteration 435000 - Loss 36.3976936340332\n",
      "Training - Iteration 436000 - Loss 36.56219482421875\n",
      "Training - Iteration 437000 - Loss 36.7626838684082\n",
      "Training - Iteration 438000 - Loss 37.08964920043945\n",
      "Training - Iteration 439000 - Loss 36.01394271850586\n",
      "Training - Iteration 440000 - Loss 36.0068473815918\n",
      "\n",
      "va, see me of one't didn't gouped uncersen one postioning as distail!\"  Moody at the monects.  Crouch freats by the me sex to move, the sefull into the impaiss oned they saybe made you, Woek her oftra\n",
      "\n",
      "Training - Iteration 441000 - Loss 35.064083099365234\n",
      "Training - Iteration 442000 - Loss 35.41669845581055\n",
      "Training - Iteration 443000 - Loss 36.18019485473633\n",
      "Training done - Best loss: 35.019981384277344\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "rnn.train(book_data, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnn.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "SEIS ,\"I - Moody's brous, had could be.  He's a hand my were of then hourned, whose too. Mr.,\n",
      "xeres behinning shock, hissed herry's haver have soon.\n",
      "Where I wear you mels say of Bulat's verm durmaming of the chair, finithous the looked.  Could's eoters arount ontiLed, his fatering bir the end, he said, by fills.  \"Whened the engmanstered, Berth  Harpleaping acread of alivingling the room.  \"Dumbledore Baster. Voldemort's behind his eyour tasked.  Mr. Balse openo.  Moody ... an attere, plee while what had leg groots, Mon Burste Bark . . ..  Might had no fute seen which was cround same was reall expe in the orts, the fried, gut and I've been than each come of tip,\" said Roges,\" said Ron's kneezzess.\n",
      "\"It's told body stand, \"de.. ement, as into our yourse.  He havin!  Onee when she people-ccouthing in his are can his head quieling regose the gryards against alst, Moody?\"\n",
      "\"She felt bly well sligm.  \"Moldoy but Marry, took, Mr. Bighing, before so hurdess. . undectily.\n",
      "Harry make there canin\n"
     ]
    }
   ],
   "source": [
    "starting_char = 'H'\n",
    "X = torch.zeros((K, 1)).to(device)\n",
    "X[char2ind[starting_char], 0] = 1\n",
    "h = torch.zeros((rnn.m, 1)).to(device)\n",
    "Y = rnn.synthesize(h, X, 1000, best=True)\n",
    "print(starting_char + ''.join([ind2char[torch.argmax(Y[:, i]).item()] for i in range(1000)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
