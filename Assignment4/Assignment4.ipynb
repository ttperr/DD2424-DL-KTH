{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 80\n"
     ]
    }
   ],
   "source": [
    "book_fname = 'data/goblet_book.txt'\n",
    "with open(book_fname, 'r') as f:\n",
    "    book_data = f.read()\n",
    "    f.close()\n",
    "\n",
    "book_chars = list(set(book_data))\n",
    "K = len(book_chars)\n",
    "print(f\"Number of unique characters: {K}\")\n",
    "char2ind, ind2char = dict(), dict()\n",
    "for i, c in enumerate(book_chars):\n",
    "    char2ind[c] = i\n",
    "    ind2char[i] = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\n",
    "    def __init__(self, m=100, seq_length=25, eta=.001, gamma=.9, sig=.01, device=device):\n",
    "        self.m = m\n",
    "        self.seq_length = seq_length\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "        self.V = torch.randn(K, m).to(self.device) * sig\n",
    "        self.c = torch.zeros(K, 1).to(self.device)\n",
    "        self.W = torch.randn(m, m).to(self.device) * sig\n",
    "        self.b = torch.zeros(m, 1).to(self.device)\n",
    "        self.U = torch.randn(m, K).to(self.device) * sig\n",
    "\n",
    "        self.V_g_ada = torch.zeros(K, m).to(self.device)\n",
    "        self.c_g_ada = torch.zeros(K, 1).to(self.device)\n",
    "        self.W_g_ada = torch.zeros(m, m).to(self.device)\n",
    "        self.b_g_ada = torch.zeros(m, 1).to(self.device)\n",
    "        self.U_g_ada = torch.zeros(m, K).to(self.device)\n",
    "\n",
    "        self.V_best = self.V.clone()\n",
    "        self.c_best = self.c.clone()\n",
    "        self.W_best = self.W.clone()\n",
    "        self.b_best = self.b.clone()\n",
    "        self.U_best = self.U.clone()\n",
    "\n",
    "        self.smooth_losses = list()\n",
    "\n",
    "        self.grads = {\n",
    "            'V': torch.zeros_like(self.V),\n",
    "            'c': torch.zeros_like(self.c),\n",
    "            'W': torch.zeros_like(self.W),\n",
    "            'b': torch.zeros_like(self.b),\n",
    "            'U': torch.zeros_like(self.U)\n",
    "        }\n",
    "\n",
    "    def synthesize(self, h_prev, x, n, best=False):\n",
    "        Y = torch.zeros((K, n)).to(self.device)\n",
    "        x_t = x\n",
    "        for i in range(n):\n",
    "            h_prev, p = self.forward(h_prev, x_t, best=best)\n",
    "            cp = torch.cumsum(p, dim=0)\n",
    "            r = torch.rand(1)\n",
    "            for j in range(K):\n",
    "                if r < cp[j]:\n",
    "                    break\n",
    "            Y[j, i] = 1\n",
    "            x_t = torch.zeros((K, 1))\n",
    "            x_t[j] = 1\n",
    "        return Y\n",
    "\n",
    "    def forward(self, h_prev, x, best=False):\n",
    "        if not best:\n",
    "            h = torch.tanh(self.W @ h_prev + self.U @ x + self.b)\n",
    "            y = self.V @ h + self.c\n",
    "            p = torch.softmax(y, dim=0)\n",
    "        else:\n",
    "            h = torch.tanh(self.W_best @ h_prev + self.U_best @ x + self.b_best)\n",
    "            y = self.V_best @ h + self.c_best\n",
    "            p = torch.softmax(y, dim=0)\n",
    "        return h, p\n",
    "    \n",
    "    def forward_pass(self, h_0, X, Y, best=False):\n",
    "        h = h_0\n",
    "        H = torch.zeros((self.m, self.seq_length + 1)).to(self.device)\n",
    "        P = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "        H[:, 0] = h.flatten()\n",
    "        loss = 0\n",
    "        for i in range(self.seq_length):\n",
    "            h, p = self.forward(h, X[:, i].reshape(K, 1), best=best)\n",
    "            H[:, i+1] = h.flatten()\n",
    "            P[:, i] = p.flatten()\n",
    "            loss += self.loss(p, Y[:, i].reshape(K, 1))\n",
    "        return H, P, loss\n",
    "\n",
    "    def backward_pass(self, H, P, X, Y):\n",
    "        self.grads = {\n",
    "            'V': torch.zeros_like(self.V),\n",
    "            'c': torch.zeros_like(self.c),\n",
    "            'W': torch.zeros_like(self.W),\n",
    "            'b': torch.zeros_like(self.b),\n",
    "            'U': torch.zeros_like(self.U)\n",
    "        }\n",
    "        dL_dh_next = torch.zeros((self.m, 1)).to(self.device)\n",
    "        h0 = H[:, 0].reshape(self.m, 1)\n",
    "        H = H[:, 1:]\n",
    "        for i in range(self.seq_length-1, -1, -1):\n",
    "            x = X[:, i].reshape(K, 1)\n",
    "            y = Y[:, i].reshape(K, 1)\n",
    "            h = H[:, i].reshape(self.m, 1)\n",
    "            p = P[:, i].reshape(K, 1)\n",
    "            g = (p - y).T\n",
    "            self.grads['V'] += g.T @ h.T\n",
    "            self.grads['c'] += g.T\n",
    "            dL_dh = self.V.T @ g.T + self.W.T @ dL_dh_next\n",
    "            dL_dh_next = dL_dh * (1 - h ** 2)\n",
    "            self.grads['W'] += dL_dh_next @ H[:, i-1].reshape(1, self.m) if i != 0 else dL_dh_next @ h0.T\n",
    "            self.grads['b'] += dL_dh_next\n",
    "            self.grads['U'] += dL_dh_next @ x.T\n",
    "    \n",
    "    def update_params(self, eps=1e-8):\n",
    "        for key in self.grads.keys():\n",
    "            self.grads[key] = torch.clamp(self.grads[key], -5, 5)\n",
    "            vars(self)[key + '_g_ada'] = self.gamma * vars(self)[key + '_g_ada'] + (1 - self.gamma) * self.grads[key] ** 2\n",
    "            vars(self)[key] -= self.eta * self.grads[key] / torch.sqrt(vars(self)[key + '_g_ada'] + eps)\n",
    "\n",
    "\n",
    "    def loss(self, p, y):\n",
    "        return - torch.sum(y.T @ torch.log(p))\n",
    "\n",
    "    def train(self, book_data, n_epochs=10):\n",
    "        n_iter = 0\n",
    "        smooth_loss = 0\n",
    "        best_loss = torch.inf\n",
    "        pbar = trange(n_epochs)\n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch {epoch}/{n_epochs}\")\n",
    "            self.e = 0\n",
    "            h = torch.zeros((self.m, 1)).to(self.device)\n",
    "            while self.e + self.seq_length <= len(book_data):\n",
    "                X_chars = book_data[self.e:self.e+self.seq_length]\n",
    "                Y_chars = book_data[self.e+1:self.e+self.seq_length+1]\n",
    "                X = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "                Y = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "                for i in range(self.seq_length):\n",
    "                    X[char2ind[X_chars[i]], i] = 1\n",
    "                    Y[char2ind[Y_chars[i]], i] = 1\n",
    "\n",
    "                H, P, loss = self.forward_pass(h, X, Y)\n",
    "                self.backward_pass(H, P, X, Y)\n",
    "                self.update_params()\n",
    "\n",
    "                smooth_loss = .999 * smooth_loss + .001 * loss if smooth_loss != 0 else loss\n",
    "                self.smooth_losses.append(smooth_loss)\n",
    "\n",
    "                if smooth_loss < best_loss:\n",
    "                    best_loss = smooth_loss\n",
    "                    self.V_best = self.V.clone()\n",
    "                    self.c_best = self.c.clone()\n",
    "                    self.W_best = self.W.clone()\n",
    "                    self.b_best = self.b.clone()\n",
    "                    self.U_best = self.U.clone()\n",
    "\n",
    "                h = H[:, -1].reshape(self.m, 1)\n",
    "\n",
    "                n_iter += 1\n",
    "                if n_iter % 1000 == 0:\n",
    "                    print(\n",
    "                        f\"Training - Iteration {n_iter} - Loss {smooth_loss}\")\n",
    "                    \n",
    "                if n_iter % 10000 == 0:\n",
    "                    Y = self.synthesize(h, X[:, 0].reshape(K, 1), 200)\n",
    "                    print()\n",
    "                    print(''.join([ind2char[torch.argmax(Y[:, i]).item()] for i in range(200)]))\n",
    "                    print()\n",
    "\n",
    "                self.e += self.seq_length\n",
    "\n",
    "        print(f\"Training done - Best loss: {best_loss}\")\n",
    "\n",
    "    def check_grads(self, book_data):\n",
    "        for key in self.grads.keys():\n",
    "            vars(self)[key].requires_grad = True\n",
    "\n",
    "        X_chars = book_data[:self.seq_length]\n",
    "        Y_chars = book_data[1:self.seq_length+1]\n",
    "        X = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "        Y = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "        h0 = torch.zeros((self.m, 1)).to(self.device)\n",
    "        for i in range(self.seq_length):\n",
    "            X[char2ind[X_chars[i]], i] = 1\n",
    "            Y[char2ind[Y_chars[i]], i] = 1\n",
    "\n",
    "        H, P, loss = self.forward_pass(h0, X, Y)\n",
    "        self.backward_pass(H, P, X, Y)\n",
    "\n",
    "        for key in self.grads.keys():\n",
    "            vars(self)[key].retain_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        print(\"Checking gradients\")\n",
    "        with torch.no_grad():\n",
    "            for key in self.grads.keys():\n",
    "                diff = torch.norm(self.grads[key] - vars(self)[key].grad)\n",
    "                rel_err = diff / (torch.norm(self.grads[key]) + torch.norm(vars(self)[key].grad) + 1e-16)\n",
    "                print(f\"Relative error on {key}: {rel_err}\")\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.smooth_losses)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Smoothed loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradients\n",
      "Relative error on V: 2.5516257551316812e-08\n",
      "Relative error on c: 3.1791032029104827e-08\n",
      "Relative error on W: 4.424012800541277e-08\n",
      "Relative error on b: 3.4715078811586864e-08\n",
      "Relative error on U: 3.0388012106641327e-08\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "rnn.check_grads(book_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efcd00e91c9464d82cb233c2ef92b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Iteration 1000 - Loss 84.24075317382812\n",
      "Training - Iteration 2000 - Loss 70.29157257080078\n",
      "Training - Iteration 3000 - Loss 63.131553649902344\n",
      "Training - Iteration 4000 - Loss 60.006046295166016\n",
      "Training - Iteration 5000 - Loss 58.24372482299805\n",
      "Training - Iteration 6000 - Loss 57.70429611206055\n",
      "Training - Iteration 7000 - Loss 57.407562255859375\n",
      "Training - Iteration 8000 - Loss 55.144195556640625\n",
      "Training - Iteration 9000 - Loss 54.24985885620117\n",
      "Training - Iteration 10000 - Loss 53.96211624145508\n",
      "\n",
      "and this cound aile lenre, ntee afri, paryut-t unintey, Herpiingn enom. MRd her, ad ugmmelle way toup. Headd,y's vin sarm as dinged late gicitt you \"u biono the mers to as blour to Ron l ont oond a'd \n",
      "\n",
      "Training - Iteration 11000 - Loss 54.546627044677734\n",
      "Training - Iteration 12000 - Loss 53.84162139892578\n",
      "Training - Iteration 13000 - Loss 53.02122497558594\n",
      "Training - Iteration 14000 - Loss 52.165767669677734\n",
      "Training - Iteration 15000 - Loss 52.1815185546875\n",
      "Training - Iteration 16000 - Loss 51.0587158203125\n",
      "Training - Iteration 17000 - Loss 50.83036804199219\n",
      "Training - Iteration 18000 - Loss 50.887203216552734\n",
      "Training - Iteration 19000 - Loss 50.509395599365234\n",
      "Training - Iteration 20000 - Loss 49.532203674316406\n",
      "\n",
      " shey deracken dich stora the daic. . had with yorred.  a?\" sidirsen anking. me, he inRon't blutba snat loak emione.  He wid spact cot snator.  Harryought arout me to edring.  He waw , torkendale of h\n",
      "\n",
      "Training - Iteration 21000 - Loss 49.132843017578125\n",
      "Training - Iteration 22000 - Loss 48.939842224121094\n",
      "Training - Iteration 23000 - Loss 49.070255279541016\n",
      "Training - Iteration 24000 - Loss 49.12570571899414\n",
      "Training - Iteration 25000 - Loss 49.130210876464844\n",
      "Training - Iteration 26000 - Loss 48.9540901184082\n",
      "Training - Iteration 27000 - Loss 48.587520599365234\n",
      "Training - Iteration 28000 - Loss 48.65170669555664\n",
      "Training - Iteration 29000 - Loss 47.99079132080078\n",
      "Training - Iteration 30000 - Loss 47.71593475341797\n",
      "\n",
      "that brich speating to and the tows butfef a .\n",
      "leed pot anl theul . . . \"cading in the rearl und set and mulledo shat \" . leod him, .hI g inound what firged od at\"\n",
      "Harry pebppess the wher there sound \n",
      "\n",
      "Training - Iteration 31000 - Loss 48.13043975830078\n",
      "Training - Iteration 32000 - Loss 47.49064636230469\n",
      "Training - Iteration 33000 - Loss 47.63444900512695\n",
      "Training - Iteration 34000 - Loss 46.5251579284668\n",
      "Training - Iteration 35000 - Loss 46.28083038330078\n",
      "Training - Iteration 36000 - Loss 46.563114166259766\n",
      "Training - Iteration 37000 - Loss 46.86872100830078\n",
      "Training - Iteration 38000 - Loss 46.44089126586914\n",
      "Training - Iteration 39000 - Loss 45.7500114440918\n",
      "Training - Iteration 40000 - Loss 45.668792724609375\n",
      "\n",
      "he pirabustraind the be there chulled of one what of ever the groundffict that they and heodddege wans from up go d,\"bon todmight o dert had in the gremort goldem -s feared dreg my to at and frof the \n",
      "\n",
      "Training - Iteration 41000 - Loss 44.844791412353516\n",
      "Training - Iteration 42000 - Loss 44.60456085205078\n",
      "Training - Iteration 43000 - Loss 45.024044036865234\n",
      "Training - Iteration 44000 - Loss 45.21113586425781\n",
      "Training - Iteration 45000 - Loss 46.84461212158203\n",
      "Training - Iteration 46000 - Loss 46.78397750854492\n",
      "Training - Iteration 47000 - Loss 47.027381896972656\n",
      "Training - Iteration 48000 - Loss 46.57394790649414\n",
      "Training - Iteration 49000 - Loss 46.62126541137695\n",
      "Training - Iteration 50000 - Loss 47.490699768066406\n",
      "\n",
      "had they was dot the eills if that Haidy iver and their of scoats anred it wearing his the off look everybard, and den uld.p sther thou didn't the has faetch\n",
      "reher,\" said Ron.\n",
      "\"sair the frot.  \"no kel\n",
      "\n",
      "Training - Iteration 51000 - Loss 48.51768112182617\n",
      "Training - Iteration 52000 - Loss 46.458858489990234\n",
      "Training - Iteration 53000 - Loss 45.111427307128906\n",
      "Training - Iteration 54000 - Loss 45.71510696411133\n",
      "Training - Iteration 55000 - Loss 48.0933723449707\n",
      "Training - Iteration 56000 - Loss 47.73945999145508\n",
      "Training - Iteration 57000 - Loss 47.25968933105469\n",
      "Training - Iteration 58000 - Loss 45.94279098510742\n",
      "Training - Iteration 59000 - Loss 46.99348449707031\n",
      "Training - Iteration 60000 - Loss 45.573970794677734\n",
      "\n",
      "und, wo look a camessffiring the Harryill her a tisHly.\"\n",
      "iacs intome on a. ereaphenome studentryed whe a went beht Dave foo nosgint suypal de, work wale this. y ted from look, fihing slet.\n",
      "\"arial of t\n",
      "\n",
      "Training - Iteration 61000 - Loss 45.423709869384766\n",
      "Training - Iteration 62000 - Loss 44.97880172729492\n",
      "Training - Iteration 63000 - Loss 45.77021026611328\n",
      "Training - Iteration 64000 - Loss 45.16071701049805\n",
      "Training - Iteration 65000 - Loss 44.7589111328125\n",
      "Training - Iteration 66000 - Loss 44.164791107177734\n",
      "Training - Iteration 67000 - Loss 45.345130920410156\n",
      "Training - Iteration 68000 - Loss 45.50447082519531\n",
      "Training - Iteration 69000 - Loss 45.650856018066406\n",
      "Training - Iteration 70000 - Loss 45.67765426635742\n",
      "\n",
      "aginar noo hin funtt.\n",
      "\"Senowed him ret ene Harry very pack to pudmainos.\"\n",
      " you reagh. \"Dewant'm to path stepens fains his ton man alk her, put the Doack beceding masien roble.  ,\" said ly. \". coom the\n",
      "\n",
      "Training - Iteration 71000 - Loss 45.57022476196289\n",
      "Training - Iteration 72000 - Loss 45.49558639526367\n",
      "Training - Iteration 73000 - Loss 44.85184097290039\n",
      "Training - Iteration 74000 - Loss 44.97837448120117\n",
      "Training - Iteration 75000 - Loss 44.69852828979492\n",
      "Training - Iteration 76000 - Loss 44.74761962890625\n",
      "Training - Iteration 77000 - Loss 44.78750991821289\n",
      "Training - Iteration 78000 - Loss 44.27052688598633\n",
      "Training - Iteration 79000 - Loss 43.11960220336914\n",
      "Training - Iteration 80000 - Loss 43.55967330932617\n",
      "\n",
      "e ugunne left reached bristwing of ta your at for acking a prouc, I  dourubated looked apparef a. use voice.  ntakent the chat's fome, he doing.  purying every of ehess gaine of sidenedk rot, exudgand\n",
      "\n",
      "Training - Iteration 81000 - Loss 43.15695571899414\n",
      "Training - Iteration 82000 - Loss 43.81353759765625\n",
      "Training - Iteration 83000 - Loss 43.059303283691406\n",
      "Training - Iteration 84000 - Loss 44.06098175048828\n",
      "Training - Iteration 85000 - Loss 42.60597229003906\n",
      "Training - Iteration 86000 - Loss 42.68505096435547\n",
      "Training - Iteration 87000 - Loss 43.080631256103516\n",
      "Training - Iteration 88000 - Loss 42.70741271972656\n",
      "Training - Iteration 89000 - Loss 44.2945671081543\n",
      "Training - Iteration 90000 - Loss 44.11825180053711\n",
      "\n",
      "o ntaside.  st. coretechore im thought. Hadry's could them wordes the woy it so be nom.\n",
      "\the had with of boge that the car sent sust to mith romso made by tho coald resn afued. your, ever and gave  ont\n",
      "\n",
      "Training - Iteration 91000 - Loss 45.06508255004883\n",
      "Training - Iteration 92000 - Loss 44.67259979248047\n",
      "Training - Iteration 93000 - Loss 44.7911262512207\n",
      "Training - Iteration 94000 - Loss 45.5964241027832\n",
      "Training - Iteration 95000 - Loss 46.58498764038086\n",
      "Training - Iteration 96000 - Loss 45.32172393798828\n",
      "Training - Iteration 97000 - Loss 43.638572692871094\n",
      "Training - Iteration 98000 - Loss 44.244930267333984\n",
      "Training - Iteration 99000 - Loss 45.17955017089844\n",
      "Training - Iteration 100000 - Loss 45.62380599975586\n",
      "\n",
      "legal, and liff mints on eHin.\" he haven.  \"for willesked hoprr\" Alouch stakneds mestored we neimpled, socion,\" ssime waroud hepe strigil, what you've suminstill eare graif, into mans hamber looking c\n",
      "\n",
      "Training - Iteration 101000 - Loss 45.80725860595703\n",
      "Training - Iteration 102000 - Loss 44.63689422607422\n",
      "Training - Iteration 103000 - Loss 45.57685089111328\n",
      "Training - Iteration 104000 - Loss 44.35013961791992\n",
      "Training - Iteration 105000 - Loss 43.662269592285156\n",
      "Training - Iteration 106000 - Loss 43.4613151550293\n",
      "Training - Iteration 107000 - Loss 44.3458251953125\n",
      "Training - Iteration 108000 - Loss 43.82876205444336\n",
      "Training - Iteration 109000 - Loss 43.701908111572266\n",
      "Training - Iteration 110000 - Loss 43.161808013916016\n",
      "\n",
      "nter into le de one and the.  Houseed at the grout the Harry heardly, an to firingu's neto she hear nor hears arm if you now knows a salleds traway, and hey he tooute them. . . thete was beckrafon fic\n",
      "\n",
      "Training - Iteration 111000 - Loss 44.07437515258789\n",
      "Training - Iteration 112000 - Loss 43.99749755859375\n",
      "Training - Iteration 113000 - Loss 44.242618560791016\n",
      "Training - Iteration 114000 - Loss 44.29465103149414\n",
      "Training - Iteration 115000 - Loss 44.647605895996094\n",
      "Training - Iteration 116000 - Loss 43.75510787963867\n",
      "Training - Iteration 117000 - Loss 44.355690002441406\n",
      "Training - Iteration 118000 - Loss 44.196224212646484\n",
      "Training - Iteration 119000 - Loss 42.99400329589844\n",
      "Training - Iteration 120000 - Loss 43.7470817565918\n",
      "\n",
      "  eot his poention at head the hmbped was craired here them, Harry's dilling her, keem.\n",
      "\" seet swanthen, and said Snethed tollou scriend of them inting guive enething had to taher of the other, artrou\n",
      "\n",
      "Training - Iteration 121000 - Loss 43.25344467163086\n",
      "Training - Iteration 122000 - Loss 43.83018112182617\n",
      "Training - Iteration 123000 - Loss 41.993370056152344\n",
      "Training - Iteration 124000 - Loss 41.907386779785156\n",
      "Training - Iteration 125000 - Loss 42.325714111328125\n",
      "Training - Iteration 126000 - Loss 43.063140869140625\n",
      "Training - Iteration 127000 - Loss 42.76726150512695\n",
      "Training - Iteration 128000 - Loss 43.237815856933594\n",
      "Training - Iteration 129000 - Loss 41.69942092895508\n",
      "Training - Iteration 130000 - Loss 42.253456115722656\n",
      "\n",
      "ticeser stopiing, thonuirs.  He had for all whilking to for getor , al my eye  ruth cuetly fater.  neveving them she wreath the dlying him it up the loe and itcdperon, speroly her bole of siinouse lea\n",
      "\n",
      "Training - Iteration 131000 - Loss 41.33647537231445\n",
      "Training - Iteration 132000 - Loss 41.89406967163086\n",
      "Training - Iteration 133000 - Loss 43.27626037597656\n",
      "Training - Iteration 134000 - Loss 43.122047424316406\n",
      "Training - Iteration 135000 - Loss 43.89603042602539\n",
      "Training - Iteration 136000 - Loss 43.75714111328125\n",
      "Training - Iteration 137000 - Loss 44.03293228149414\n",
      "Training - Iteration 138000 - Loss 44.08090591430664\n",
      "Training - Iteration 139000 - Loss 44.902950286865234\n",
      "Training - Iteration 140000 - Loss 45.616268157958984\n",
      "\n",
      "grept rescanded at whistech, happent, as though of grinned ampersed as twoshenercle.s whisd gitedla. shainal deftly, they couldn't have on the string the teached the Bulginy dars with viiwititahing ue\n",
      "\n",
      "Training - Iteration 141000 - Loss 43.6174201965332\n",
      "Training - Iteration 142000 - Loss 43.36595153808594\n",
      "Training - Iteration 143000 - Loss 43.87655258178711\n",
      "Training - Iteration 144000 - Loss 45.474361419677734\n",
      "Training - Iteration 145000 - Loss 45.30986404418945\n",
      "Training - Iteration 146000 - Loss 44.15537643432617\n",
      "Training - Iteration 147000 - Loss 45.00802993774414\n",
      "Training - Iteration 148000 - Loss 44.196922302246094\n",
      "Training - Iteration 149000 - Loss 43.02359390258789\n",
      "Training - Iteration 150000 - Loss 42.8740234375\n",
      "\n",
      "time anyone ablept de.  deen mole haps hoochour \"at the and it.  Whust yeerle the rarmartlice.\n",
      "\", all to get feeme, \"il and sounder,\" had kening scinterating his actice youch. \"I'm ,\" ke, impre\n",
      "chaele\n",
      "\n",
      "Training - Iteration 151000 - Loss 43.40347671508789\n",
      "Training - Iteration 152000 - Loss 43.566524505615234\n",
      "Training - Iteration 153000 - Loss 42.895198822021484\n",
      "Training - Iteration 154000 - Loss 42.49946594238281\n",
      "Training - Iteration 155000 - Loss 42.86652755737305\n",
      "Training - Iteration 156000 - Loss 43.170345306396484\n",
      "Training - Iteration 157000 - Loss 43.225433349609375\n",
      "Training - Iteration 158000 - Loss 43.64459991455078\n",
      "Training - Iteration 159000 - Loss 43.828887939453125\n",
      "Training - Iteration 160000 - Loss 43.46343231201172\n",
      "\n",
      "mind.  issued the way Hermaring for theilly, walking a selie.  a \n",
      "\"Harry saying at say\" walk sunce.\n",
      "\"get gonieen,\" said Hermione, you rook the and sack, cleally.  Harry, startedbah,\" whossive noo like\n",
      "\n",
      "Training - Iteration 161000 - Loss 43.7225341796875\n",
      "Training - Iteration 162000 - Loss 43.21242904663086\n",
      "Training - Iteration 163000 - Loss 43.155277252197266\n",
      "Training - Iteration 164000 - Loss 43.779563903808594\n",
      "Training - Iteration 165000 - Loss 42.96718978881836\n",
      "Training - Iteration 166000 - Loss 43.35200500488281\n",
      "Training - Iteration 167000 - Loss 41.995540618896484\n",
      "Training - Iteration 168000 - Loss 42.18858337402344\n",
      "Training - Iteration 169000 - Loss 42.17359924316406\n",
      "Training - Iteration 170000 - Loss 42.903564453125\n",
      "\n",
      "ssed the sor ovenH.  \"chat ane. Herminnate the came, satoly had gringf.\"\n",
      "\"mashing, tah the abled alsitt the milatate. \"Then her scappamed which daes hally fremhoined would sor uther siired, give abfe \n",
      "\n",
      "Training - Iteration 171000 - Loss 42.57727813720703\n",
      "Training - Iteration 172000 - Loss 42.34674072265625\n",
      "Training - Iteration 173000 - Loss 41.67523956298828\n",
      "Training - Iteration 174000 - Loss 41.48908615112305\n",
      "Training - Iteration 175000 - Loss 40.860694885253906\n",
      "Training - Iteration 176000 - Loss 41.327022552490234\n",
      "Training - Iteration 177000 - Loss 42.0054817199707\n",
      "Training - Iteration 178000 - Loss 43.46941375732422\n",
      "Training - Iteration 179000 - Loss 43.643646240234375\n",
      "Training - Iteration 180000 - Loss 43.61717224121094\n",
      "\n",
      "u glauret the kit  de nomes.\n",
      "\"wagain. it maring him was the The reat that mead him sudrippes away becaurhe he haveng at mecked unite wive ad on goblethed eif and anvith Unnor's.f the would the eall th\n",
      "\n",
      "Training - Iteration 181000 - Loss 43.668174743652344\n",
      "Training - Iteration 182000 - Loss 43.622047424316406\n",
      "Training - Iteration 183000 - Loss 44.64045333862305\n",
      "Training - Iteration 184000 - Loss 45.6025505065918\n",
      "Training - Iteration 185000 - Loss 43.47547912597656\n",
      "Training - Iteration 186000 - Loss 42.70735168457031\n",
      "Training - Iteration 187000 - Loss 43.241455078125\n",
      "Training - Iteration 188000 - Loss 45.78023910522461\n",
      "Training - Iteration 189000 - Loss 45.29715347290039\n",
      "Training - Iteration 190000 - Loss 44.363800048828125\n",
      "\n",
      "alt.\"\n",
      "and to rievers like  it deat die the's sevent had's larder, braways laughy of hindeling all, . \"was. rativyiblepy ,inself to stair, whonger,\" eollfs, liht roneach coae time. . . e alher into the\n",
      "\n",
      "Training - Iteration 191000 - Loss 43.48002243041992\n",
      "Training - Iteration 192000 - Loss 44.310115814208984\n",
      "Training - Iteration 193000 - Loss 42.99053192138672\n",
      "Training - Iteration 194000 - Loss 42.74871063232422\n",
      "Training - Iteration 195000 - Loss 42.57020568847656\n",
      "Training - Iteration 196000 - Loss 43.4197883605957\n",
      "Training - Iteration 197000 - Loss 42.929222106933594\n",
      "Training - Iteration 198000 - Loss 42.516971588134766\n",
      "Training - Iteration 199000 - Loss 42.021759033203125\n",
      "Training - Iteration 200000 - Loss 43.195892333984375\n",
      "\n",
      " too seee  \"heldled.  Hermione  reut had enolies briound or his hand fol every conder as Hermione blaid, becule fregan ere aothnted.  the nifuse's goous farts and Harry but towngtupate tingers most pr\n",
      "\n",
      "Training - Iteration 201000 - Loss 43.41648864746094\n",
      "Training - Iteration 202000 - Loss 43.4931526184082\n",
      "Training - Iteration 203000 - Loss 43.78657531738281\n",
      "Training - Iteration 204000 - Loss 43.24784851074219\n",
      "Training - Iteration 205000 - Loss 43.52970504760742\n",
      "Training - Iteration 206000 - Loss 42.750301361083984\n",
      "Training - Iteration 207000 - Loss 43.058902740478516\n",
      "Training - Iteration 208000 - Loss 43.037841796875\n",
      "Training - Iteration 209000 - Loss 42.86550521850586\n",
      "Training - Iteration 210000 - Loss 43.134605407714844\n",
      "\n",
      " seeperit, heststinilaoftleptly her looking, why esfficouse rook.\n",
      "\"makible. \n",
      "\"His ratch at the elsticing that s,\" said Haro.  ufting Harry, what's up the awarnf aidleadh?\"  said Harry, miade dare sat,\n",
      "\n",
      "Training - Iteration 211000 - Loss 41.99706268310547\n",
      "Training - Iteration 212000 - Loss 41.519622802734375\n",
      "Training - Iteration 213000 - Loss 42.02447509765625\n",
      "Training - Iteration 214000 - Loss 41.45798110961914\n",
      "Training - Iteration 215000 - Loss 42.073455810546875\n",
      "Training - Iteration 216000 - Loss 41.542877197265625\n",
      "Training - Iteration 217000 - Loss 42.30526351928711\n",
      "Training - Iteration 218000 - Loss 41.221031188964844\n",
      "Training - Iteration 219000 - Loss 41.10025405883789\n",
      "Training - Iteration 220000 - Loss 41.28064727783203\n",
      "\n",
      "hed around down as, each kispanoam, eyear you mantermentding low with his feel dfilled, pplate in the breath.  heal your thath lor thistered again, not was went rid usting and, comp oastenced to see w\n",
      "\n",
      "Training - Iteration 221000 - Loss 41.420745849609375\n",
      "Training - Iteration 222000 - Loss 43.052066802978516\n",
      "Training - Iteration 223000 - Loss 42.55323028564453\n",
      "Training - Iteration 224000 - Loss 43.53462219238281\n",
      "Training - Iteration 225000 - Loss 43.107078552246094\n",
      "Training - Iteration 226000 - Loss 43.608604431152344\n",
      "Training - Iteration 227000 - Loss 44.33020782470703\n",
      "Training - Iteration 228000 - Loss 45.41892623901367\n",
      "Training - Iteration 229000 - Loss 43.81193161010742\n",
      "Training - Iteration 230000 - Loss 41.93568420410156\n",
      "\n",
      "lly. \"I  looked ty, a remeofilly thrniened word.odred to made a tow it campared broulded vitchenedrescalet.\n",
      "\"wathen through the motion. \"I'm they smmith back, other.\"\n",
      " a said Mr. Weading wizard when t\n",
      "\n",
      "Training - Iteration 231000 - Loss 42.89704513549805\n",
      "Training - Iteration 232000 - Loss 43.61278533935547\n",
      "Training - Iteration 233000 - Loss 44.334556579589844\n",
      "Training - Iteration 234000 - Loss 44.57376480102539\n",
      "Training - Iteration 235000 - Loss 43.38995361328125\n",
      "Training - Iteration 236000 - Loss 44.28274154663086\n",
      "Training - Iteration 237000 - Loss 42.61111068725586\n",
      "Training - Iteration 238000 - Loss 42.39637756347656\n",
      "Training - Iteration 239000 - Loss 41.96468734741211\n",
      "Training - Iteration 240000 - Loss 42.9327392578125\n",
      "\n",
      "the same of th not hem mamse, thild mams ard ve gotd as she old him beescal hought fair and of furing eye tile, spIfter comitdle, seat.\n",
      "\"He basn't getedore are out,. Harry, hasperaged rises.  supter. \n",
      "\n",
      "Training - Iteration 241000 - Loss 42.33259963989258\n",
      "Training - Iteration 242000 - Loss 42.22586441040039\n",
      "Training - Iteration 243000 - Loss 41.825035095214844\n",
      "Training - Iteration 244000 - Loss 43.142494201660156\n",
      "Training - Iteration 245000 - Loss 42.783714294433594\n",
      "Training - Iteration 246000 - Loss 43.11350631713867\n",
      "Training - Iteration 247000 - Loss 43.23936462402344\n",
      "Training - Iteration 248000 - Loss 43.89727020263672\n",
      "Training - Iteration 249000 - Loss 42.93563461303711\n",
      "Training - Iteration 250000 - Loss 43.262813568115234\n",
      "\n",
      "ting ine off with any stared his in the stauden morndomet.  \"dom a feurs in a hand this to geings effore bre ither had felch now ingIt that had looking nonding her mirck thack to the sile oth heard ve\n",
      "\n",
      "Training - Iteration 251000 - Loss 43.061622619628906\n",
      "Training - Iteration 252000 - Loss 42.19089889526367\n",
      "Training - Iteration 253000 - Loss 42.81210708618164\n",
      "Training - Iteration 254000 - Loss 42.26520538330078\n",
      "Training - Iteration 255000 - Loss 42.561317443847656\n",
      "Training - Iteration 256000 - Loss 40.85417938232422\n",
      "Training - Iteration 257000 - Loss 41.027183532714844\n",
      "Training - Iteration 258000 - Loss 40.934043884277344\n",
      "Training - Iteration 259000 - Loss 41.9265251159668\n",
      "Training - Iteration 260000 - Loss 41.64765167236328\n",
      "\n",
      "tes and whill Harry's ey him the wouedsmeas becing HerppistIkerHarry Harr and speaking that he thought out a head trewid, was not ar his not was standly, from then he saids are agwins.  Harry couldn't\n",
      "\n",
      "Training - Iteration 261000 - Loss 42.270729064941406\n",
      "Training - Iteration 262000 - Loss 40.6948127746582\n",
      "Training - Iteration 263000 - Loss 41.19365692138672\n",
      "Training - Iteration 264000 - Loss 40.7528076171875\n",
      "Training - Iteration 265000 - Loss 40.92955780029297\n",
      "Training - Iteration 266000 - Loss 42.76201629638672\n",
      "Training - Iteration 267000 - Loss 42.38432693481445\n",
      "Training - Iteration 268000 - Loss 43.031375885009766\n",
      "Training - Iteration 269000 - Loss 42.710655212402344\n",
      "Training - Iteration 270000 - Loss 43.18559265136719\n",
      "\n",
      "of and ar a sleek as though a stanted thee could when he hading.  lon't have bit a geber, very alang a wonise werlly, bo the toon fillesstving been poth.  \"avers the morst a case ertangeatibaly.\n",
      "\" tot\n",
      "\n",
      "Training - Iteration 271000 - Loss 43.41322708129883\n",
      "Training - Iteration 272000 - Loss 44.236663818359375\n",
      "Training - Iteration 273000 - Loss 44.50575637817383\n",
      "Training - Iteration 274000 - Loss 42.600337982177734\n",
      "Training - Iteration 275000 - Loss 42.77787780761719\n",
      "Training - Iteration 276000 - Loss 43.06147384643555\n",
      "Training - Iteration 277000 - Loss 44.510318756103516\n",
      "Training - Iteration 278000 - Loss 44.713478088378906\n",
      "Training - Iteration 279000 - Loss 43.10907745361328\n",
      "Training - Iteration 280000 - Loss 44.271060943603516\n",
      "\n",
      "arris, see in to of ind mes, and worts.t most ter saly of still very controry waware tearnars.  or sile. \"and at there's.\n",
      " ant appuden, I amso peetertatcying to gring billing to toe stof timp sepwert \n",
      "\n",
      "Training - Iteration 281000 - Loss 43.35050582885742\n",
      "Training - Iteration 282000 - Loss 42.182132720947266\n",
      "Training - Iteration 283000 - Loss 41.913726806640625\n",
      "Training - Iteration 284000 - Loss 42.69512939453125\n",
      "Training - Iteration 285000 - Loss 42.63201141357422\n",
      "Training - Iteration 286000 - Loss 41.926456451416016\n",
      "Training - Iteration 287000 - Loss 41.6974983215332\n",
      "Training - Iteration 288000 - Loss 42.45222854614258\n",
      "Training - Iteration 289000 - Loss 42.7639274597168\n",
      "Training - Iteration 290000 - Loss 42.6738166809082\n",
      "\n",
      "on himping voff,  Ronyon's swaty haid eyes larur you the panged were bewnmsoring to the smalling the secoiding him ooe hur.f gave knowor with ever a,\" said Ron pagman tablates sure nasonry in then  ya\n",
      "\n",
      "Training - Iteration 291000 - Loss 43.268672943115234\n",
      "Training - Iteration 292000 - Loss 42.65095901489258\n",
      "Training - Iteration 293000 - Loss 42.68073272705078\n",
      "Training - Iteration 294000 - Loss 43.015403747558594\n",
      "Training - Iteration 295000 - Loss 42.56993865966797\n",
      "Training - Iteration 296000 - Loss 42.324790954589844\n",
      "Training - Iteration 297000 - Loss 42.96995162963867\n",
      "Training - Iteration 298000 - Loss 42.133697509765625\n",
      "Training - Iteration 299000 - Loss 42.79218673706055\n",
      "Training - Iteration 300000 - Loss 41.25632095336914\n",
      "\n",
      "n magicale and moke.  He wassele,h grinnions,\" said Hermiones straiged a so hading on twan woye actapil, and right his wand bother doly asked.\n",
      "\". I knrigh out was betting he wald at you kaid Harryon w\n",
      "\n",
      "Training - Iteration 301000 - Loss 41.34840774536133\n",
      "Training - Iteration 302000 - Loss 41.47963333129883\n",
      "Training - Iteration 303000 - Loss 42.160640716552734\n",
      "Training - Iteration 304000 - Loss 41.862545013427734\n",
      "Training - Iteration 305000 - Loss 42.05369567871094\n",
      "Training - Iteration 306000 - Loss 40.848453521728516\n",
      "Training - Iteration 307000 - Loss 40.97948455810547\n",
      "Training - Iteration 308000 - Loss 40.3862419128418\n",
      "Training - Iteration 309000 - Loss 40.73932647705078\n",
      "Training - Iteration 310000 - Loss 41.53243637084961\n",
      "\n",
      "nda likeny on had was, we has.  artacsmed he fight, got, the windrowh,h  ere wit in off as th. \n",
      "\"\n",
      "none tious.  epen that him back of  merted himay.\"\n",
      "\"but the tryTher manmentson,\"\n",
      "\"Thi've cared still t\n",
      "\n",
      "Training - Iteration 311000 - Loss 42.71243667602539\n",
      "Training - Iteration 312000 - Loss 43.303287506103516\n",
      "Training - Iteration 313000 - Loss 42.70062255859375\n",
      "Training - Iteration 314000 - Loss 43.05459213256836\n",
      "Training - Iteration 315000 - Loss 43.32900619506836\n",
      "Training - Iteration 316000 - Loss 44.17567825317383\n",
      "Training - Iteration 317000 - Loss 45.20863723754883\n",
      "Training - Iteration 318000 - Loss 42.748626708984375\n",
      "Training - Iteration 319000 - Loss 42.37065505981445\n",
      "Training - Iteration 320000 - Loss 42.34029769897461\n",
      "\n",
      "inkly.  \"Wsy with moment, a going mamp said, heard hergelp.. shall of this humplys e a were tolriguing ity --\"nhen through out off indowing the poping one,  your in of fiffert heard did beanhalit away\n",
      "\n",
      "Training - Iteration 321000 - Loss 44.90068435668945\n",
      "Training - Iteration 322000 - Loss 44.473114013671875\n",
      "Training - Iteration 323000 - Loss 43.52553939819336\n",
      "Training - Iteration 324000 - Loss 42.893489837646484\n",
      "Training - Iteration 325000 - Loss 43.74551010131836\n",
      "Training - Iteration 326000 - Loss 42.517608642578125\n",
      "Training - Iteration 327000 - Loss 42.0337028503418\n",
      "Training - Iteration 328000 - Loss 42.20348358154297\n",
      "Training - Iteration 329000 - Loss 42.66172790527344\n",
      "Training - Iteration 330000 - Loss 42.07073974609375\n",
      "\n",
      "me, you're ubble, gire names to vignly stile ens.  Harry forwary dim of thoughth very.  any that wagted the olto doltws enselase.  back, rfwon room to read becogmer smooksher caohing three's ponita, d\n",
      "\n",
      "Training - Iteration 331000 - Loss 41.740787506103516\n",
      "Training - Iteration 332000 - Loss 41.80830001831055\n",
      "Training - Iteration 333000 - Loss 42.44267654418945\n",
      "Training - Iteration 334000 - Loss 42.940673828125\n",
      "Training - Iteration 335000 - Loss 42.731449127197266\n",
      "Training - Iteration 336000 - Loss 43.049781799316406\n",
      "Training - Iteration 337000 - Loss 42.70527267456055\n",
      "Training - Iteration 338000 - Loss 43.0894660949707\n",
      "Training - Iteration 339000 - Loss 42.33231735229492\n",
      "Training - Iteration 340000 - Loss 42.404911041259766\n",
      "\n",
      "oke of he'f looked a's ouk turnd so ficdensectuling the grow houself, s. dent rebleong foominousled vied as drefer and or his raled, his fached visebs lowesshent , the sit oware through the watered as\n",
      "\n",
      "Training - Iteration 341000 - Loss 42.952423095703125\n",
      "Training - Iteration 342000 - Loss 42.16546630859375\n",
      "Training - Iteration 343000 - Loss 42.608577728271484\n",
      "Training - Iteration 344000 - Loss 41.40034103393555\n",
      "Training - Iteration 345000 - Loss 40.722782135009766\n",
      "Training - Iteration 346000 - Loss 41.201271057128906\n",
      "Training - Iteration 347000 - Loss 41.46949768066406\n",
      "Training - Iteration 348000 - Loss 41.336814880371094\n",
      "Training - Iteration 349000 - Loss 41.05266189575195\n",
      "Training - Iteration 350000 - Loss 41.64600372314453\n",
      "\n",
      ", whith on the ...\n",
      "he was not benorek voith, stading he was not gath that, I was a opple ... hait.\n",
      "paded dagiasted Harry - do antame, mather the curwiont by the behind the was conatered to his waik, d\n",
      "\n",
      "Training - Iteration 351000 - Loss 40.8421745300293\n",
      "Training - Iteration 352000 - Loss 40.63597106933594\n",
      "Training - Iteration 353000 - Loss 40.82302474975586\n",
      "Training - Iteration 354000 - Loss 40.80408477783203\n",
      "Training - Iteration 355000 - Loss 42.712432861328125\n",
      "Training - Iteration 356000 - Loss 42.35692596435547\n",
      "Training - Iteration 357000 - Loss 43.07585906982422\n",
      "Training - Iteration 358000 - Loss 42.51504135131836\n",
      "Training - Iteration 359000 - Loss 43.03535842895508\n",
      "Training - Iteration 360000 - Loss 43.8974494934082\n",
      "\n",
      "ikees, they's ever eorride of a right carpents Ball cools of boldeemeat ded a Arland itt the blached farthall had bolmon make ouply all entrate 'opsly her as though pudey firitching them leanly \"\n",
      "\"He \n",
      "\n",
      "Training - Iteration 361000 - Loss 45.179901123046875\n",
      "Training - Iteration 362000 - Loss 43.41573715209961\n",
      "Training - Iteration 363000 - Loss 41.48289489746094\n",
      "Training - Iteration 364000 - Loss 42.36964416503906\n",
      "Training - Iteration 365000 - Loss 43.68008041381836\n",
      "Training - Iteration 366000 - Loss 43.87501525878906\n",
      "Training - Iteration 367000 - Loss 43.97654342651367\n",
      "Training - Iteration 368000 - Loss 42.90027618408203\n",
      "Training - Iteration 369000 - Loss 43.7823486328125\n",
      "Training - Iteration 370000 - Loss 42.09740447998047\n",
      "\n",
      "Gonad good, and that sfoetts and ceather, in the didn't aroudy roger.\n",
      "\"Harry, the tavense.  ut Harry po-ent.  Hit of they all silvered and got thming his out of his leep verlunce thre gotingents troug\n",
      "\n",
      "Training - Iteration 371000 - Loss 41.756317138671875\n",
      "Training - Iteration 372000 - Loss 41.544124603271484\n",
      "Training - Iteration 373000 - Loss 42.636199951171875\n",
      "Training - Iteration 374000 - Loss 41.922325134277344\n",
      "Training - Iteration 375000 - Loss 41.73683166503906\n",
      "Training - Iteration 376000 - Loss 41.455055236816406\n",
      "Training - Iteration 377000 - Loss 42.52953338623047\n",
      "Training - Iteration 378000 - Loss 42.57228469848633\n",
      "Training - Iteration 379000 - Loss 42.56135559082031\n",
      "Training - Iteration 380000 - Loss 42.79441452026367\n",
      "\n",
      "chething to him you hander.\n",
      "\"'s seathan the reath  enflet of you she so hang.\n",
      "\"\n",
      "not seether at the eld held a.  doing at the smarte.  oon,\" she rarvittming noor, and framint baesy, \"ohe had outh since\n",
      "\n",
      "Training - Iteration 381000 - Loss 43.114158630371094\n",
      "Training - Iteration 382000 - Loss 42.8149299621582\n",
      "Training - Iteration 383000 - Loss 42.543270111083984\n",
      "Training - Iteration 384000 - Loss 42.730674743652344\n",
      "Training - Iteration 385000 - Loss 41.93380355834961\n",
      "Training - Iteration 386000 - Loss 42.344390869140625\n",
      "Training - Iteration 387000 - Loss 41.653724670410156\n",
      "Training - Iteration 388000 - Loss 41.93035888671875\n",
      "Training - Iteration 389000 - Loss 40.38165283203125\n",
      "Training - Iteration 390000 - Loss 40.99922180175781\n",
      "\n",
      "maroched his housice on real office had staring simer, thed comped forather surice.\n",
      "\"don't belr matory a sellthis math him.\"\n",
      "\"the closed in enothew starbenet at the gurnes, Harry cellwiss, and he had \n",
      "\n",
      "Training - Iteration 391000 - Loss 40.59831237792969\n",
      "Training - Iteration 392000 - Loss 41.293548583984375\n",
      "Training - Iteration 393000 - Loss 40.872596740722656\n",
      "Training - Iteration 394000 - Loss 41.96889877319336\n",
      "Training - Iteration 395000 - Loss 40.39901351928711\n",
      "Training - Iteration 396000 - Loss 40.747928619384766\n",
      "Training - Iteration 397000 - Loss 40.371192932128906\n",
      "Training - Iteration 398000 - Loss 40.37651824951172\n",
      "Training - Iteration 399000 - Loss 42.40088653564453\n",
      "Training - Iteration 400000 - Loss 41.871604919433594\n",
      "\n",
      "on, bean, and thereu to mort hearding you , a streached about eicuble, and in his frevt out a  moint in still could surtereaded startly  of usinh over the fough his oke around in her.\n",
      "\t, oking.  leing\n",
      "\n",
      "Training - Iteration 401000 - Loss 42.6876335144043\n",
      "Training - Iteration 402000 - Loss 42.300132751464844\n",
      "Training - Iteration 403000 - Loss 42.69575119018555\n",
      "Training - Iteration 404000 - Loss 43.28557586669922\n",
      "Training - Iteration 405000 - Loss 43.84257888793945\n",
      "Training - Iteration 406000 - Loss 43.65907287597656\n",
      "Training - Iteration 407000 - Loss 42.03900909423828\n",
      "Training - Iteration 408000 - Loss 42.251216888427734\n",
      "Training - Iteration 409000 - Loss 42.899391174316406\n",
      "Training - Iteration 410000 - Loss 43.79928207397461\n",
      "\n",
      "lent ow at know to did for the didnvouting go.  \"oise reticionalp to anyoskoations like to the skrily in trripushiare.  etuling out. Hikily, and awarley, handerent yalo do agled floomed iture.  \"Pots \n",
      "\n",
      "Training - Iteration 411000 - Loss 44.20748519897461\n",
      "Training - Iteration 412000 - Loss 42.710819244384766\n",
      "Training - Iteration 413000 - Loss 43.88163375854492\n",
      "Training - Iteration 414000 - Loss 42.64472961425781\n",
      "Training - Iteration 415000 - Loss 41.62247085571289\n",
      "Training - Iteration 416000 - Loss 41.48178482055664\n",
      "Training - Iteration 417000 - Loss 42.344581604003906\n",
      "Training - Iteration 418000 - Loss 42.02372741699219\n",
      "Training - Iteration 419000 - Loss 41.631080627441406\n",
      "Training - Iteration 420000 - Loss 41.382354736328125\n",
      "\n",
      "re time the drep her mentwalacing the right. . weand-uther had gold invi was commorvallt e srcitold ap\n",
      "kisse, it what had did. malled int.  .\". thereb's great, \"poor offuse thist, less in a marks your\n",
      "\n",
      "Training - Iteration 421000 - Loss 42.178382873535156\n",
      "Training - Iteration 422000 - Loss 42.329158782958984\n",
      "Training - Iteration 423000 - Loss 42.349491119384766\n",
      "Training - Iteration 424000 - Loss 42.6946907043457\n",
      "Training - Iteration 425000 - Loss 42.2537956237793\n",
      "Training - Iteration 426000 - Loss 42.00416564941406\n",
      "Training - Iteration 427000 - Loss 42.90543746948242\n",
      "Training - Iteration 428000 - Loss 42.18172073364258\n",
      "Training - Iteration 429000 - Loss 41.78738784790039\n",
      "Training - Iteration 430000 - Loss 42.42302322387695\n",
      "\n",
      "he envestedane mogee wonned.  ill you constacy lusged his anyes inserthipertate him \n",
      "\"turned \"wa troach lisa dumbe monbe a gait of the apether tace boy of the dungeting. he so roob tile the bester to \n",
      "\n",
      "Training - Iteration 431000 - Loss 41.765811920166016\n",
      "Training - Iteration 432000 - Loss 42.36322784423828\n",
      "Training - Iteration 433000 - Loss 40.6411018371582\n",
      "Training - Iteration 434000 - Loss 40.7911262512207\n",
      "Training - Iteration 435000 - Loss 40.862239837646484\n",
      "Training - Iteration 436000 - Loss 41.54735565185547\n",
      "Training - Iteration 437000 - Loss 41.16346740722656\n",
      "Training - Iteration 438000 - Loss 41.65209197998047\n",
      "Training - Iteration 439000 - Loss 40.0900993347168\n",
      "Training - Iteration 440000 - Loss 40.75807189941406\n",
      "\n",
      "s I in upeniar.\n",
      "\n",
      "\"untorred voicely, and see you lat.  dang.\n",
      "\"le.   bethessort, looke his had one intoren rire one up.\n",
      "ark reatsh over rack pisseated into the gotion the thought of kimereds, every only\n",
      "\n",
      "Training - Iteration 441000 - Loss 39.8644905090332\n",
      "Training - Iteration 442000 - Loss 40.075103759765625\n",
      "Training - Iteration 443000 - Loss 40.98744583129883\n",
      "Training done - Best loss: 39.753395080566406\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "rnn.train(book_data, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnn.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Haired.\n",
      "\"one eher.\n",
      "ark Harry at Harry you a she seemed, as in eyely wa lear . . . tak essey \"Iband of it was botter mament.  Harry, hisn't ebored over to pling to Harry meas scar and evening foum rour, back.  Harry loured of echa nead, Harryes,\" said Dumbledore spron Head a rueared with to himse, he was bbeast into to wit slight, sleco have serpt of the beoind not to all and see hand to it once cound.n as . .  he had deal let reared. . . as, look a monever,\" he koitied it,\" sait making it ever deano eocore though at necause coutintry is finct perient complets at he said you wished in with eyes,  ons snaving it if the feck nose very flit you'd needs wvery comple to returned misting him noins, anman,\" said Dumbledore hurr ,\"rever armost for under no you now.  Iy arwived a with hop eearclessaining to me hi\" reaked the Hisped, hush have sparmally for it as\" grappus coul must for a nooked to me a see of him, now my moutinting rest ofhed, pece to festared ut thi\"\n",
      "\"you oid, and the wanded ahes\n"
     ]
    }
   ],
   "source": [
    "starting_char = 'H'\n",
    "X = torch.zeros((K, 1)).to(device)\n",
    "X[char2ind[starting_char], 0] = 1\n",
    "h = torch.zeros((rnn.m, 1)).to(device)\n",
    "Y = rnn.synthesize(h, X, 1000, best=True)\n",
    "print(starting_char + ''.join([ind2char[torch.argmax(Y[:, i]).item()] for i in range(1000)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
