{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 80\n"
     ]
    }
   ],
   "source": [
    "book_fname = 'data/goblet_book.txt'\n",
    "with open(book_fname, 'r') as f:\n",
    "    book_data = f.read()\n",
    "    f.close()\n",
    "\n",
    "book_chars = list(set(book_data))\n",
    "K = len(book_chars)\n",
    "print(f\"Number of unique characters: {K}\")\n",
    "char2ind, ind2char = dict(), dict()\n",
    "for i, c in enumerate(book_chars):\n",
    "    char2ind[c] = i\n",
    "    ind2char[i] = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\n",
    "    def __init__(self, m=100, seq_length=25, eta=.001, gamma=.9, sig=.01, device=device):\n",
    "        self.m = m\n",
    "        self.seq_length = seq_length\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "        self.V = torch.randn(K, m).to(self.device) * sig\n",
    "        self.c = torch.zeros(K, 1).to(self.device)\n",
    "        self.W = torch.randn(m, m).to(self.device) * sig\n",
    "        self.b = torch.zeros(m, 1).to(self.device)\n",
    "        self.U = torch.randn(m, K).to(self.device) * sig\n",
    "\n",
    "        self.V_g_ada = torch.zeros(K, m).to(self.device)\n",
    "        self.c_g_ada = torch.zeros(K, 1).to(self.device)\n",
    "        self.W_g_ada = torch.zeros(m, m).to(self.device)\n",
    "        self.b_g_ada = torch.zeros(m, 1).to(self.device)\n",
    "        self.U_g_ada = torch.zeros(m, K).to(self.device)\n",
    "\n",
    "        self.V_best = self.V.clone()\n",
    "        self.c_best = self.c.clone()\n",
    "        self.W_best = self.W.clone()\n",
    "        self.b_best = self.b.clone()\n",
    "        self.U_best = self.U.clone()\n",
    "\n",
    "        self.smooth_losses = list()\n",
    "\n",
    "        self.grads = {\n",
    "            'V': torch.zeros_like(self.V),\n",
    "            'c': torch.zeros_like(self.c),\n",
    "            'W': torch.zeros_like(self.W),\n",
    "            'b': torch.zeros_like(self.b),\n",
    "            'U': torch.zeros_like(self.U)\n",
    "        }\n",
    "\n",
    "    def synthesize(self, h_prev, x, n, best=False):\n",
    "        Y = torch.zeros((K, n)).to(self.device)\n",
    "        x_t = x\n",
    "        for i in range(n):\n",
    "            h_prev, p = self.forward(h_prev, x_t, best=best)\n",
    "            cp = torch.cumsum(p, dim=0)\n",
    "            r = torch.rand(1)\n",
    "            for j in range(K):\n",
    "                if r < cp[j]:\n",
    "                    break\n",
    "            Y[j, i] = 1\n",
    "            x_t = torch.zeros((K, 1))\n",
    "            x_t[j] = 1\n",
    "        return Y\n",
    "\n",
    "    def forward(self, h_prev, x, best=False):\n",
    "        if not best:\n",
    "            h = torch.tanh(self.W @ h_prev + self.U @ x + self.b)\n",
    "            y = self.V @ h + self.c\n",
    "            p = torch.softmax(y, dim=0)\n",
    "        else:\n",
    "            h = torch.tanh(self.W_best @ h_prev + self.U_best @ x + self.b_best)\n",
    "            y = self.V_best @ h + self.c_best\n",
    "            p = torch.softmax(y, dim=0)\n",
    "        return h, p\n",
    "    \n",
    "    def forward_pass(self, h_0, X, Y, best=False):\n",
    "        h = h_0\n",
    "        H = torch.zeros((self.m, self.seq_length + 1)).to(self.device)\n",
    "        P = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "        H[:, 0] = h.flatten()\n",
    "        loss = 0\n",
    "        for i in range(self.seq_length):\n",
    "            h, p = self.forward(h, X[:, i].reshape(K, 1), best=best)\n",
    "            H[:, i+1] = h.flatten()\n",
    "            P[:, i] = p.flatten()\n",
    "            loss += self.loss(p, Y[:, i].reshape(K, 1))\n",
    "        return H, P, loss\n",
    "\n",
    "    def backward_pass(self, H, P, X, Y):\n",
    "        self.grads = {\n",
    "            'V': torch.zeros_like(self.V),\n",
    "            'c': torch.zeros_like(self.c),\n",
    "            'W': torch.zeros_like(self.W),\n",
    "            'b': torch.zeros_like(self.b),\n",
    "            'U': torch.zeros_like(self.U)\n",
    "        }\n",
    "        dL_dh_next = torch.zeros((self.m, 1)).to(self.device)\n",
    "        h0 = H[:, 0].reshape(self.m, 1)\n",
    "        H = H[:, 1:]\n",
    "        for i in range(self.seq_length-1, -1, -1):\n",
    "            x = X[:, i].reshape(K, 1)\n",
    "            y = Y[:, i].reshape(K, 1)\n",
    "            h = H[:, i].reshape(self.m, 1)\n",
    "            p = P[:, i].reshape(K, 1)\n",
    "            g = (p - y).T\n",
    "            self.grads['V'] += g.T @ h.T\n",
    "            self.grads['c'] += g.T\n",
    "            dL_dh = self.V.T @ g.T + self.W.T @ dL_dh_next\n",
    "            dL_dh_next = dL_dh * (1 - h ** 2)\n",
    "            self.grads['W'] += dL_dh_next @ H[:, i-1].reshape(1, self.m) if i != 0 else dL_dh_next @ h0.T\n",
    "            self.grads['b'] += dL_dh_next\n",
    "            self.grads['U'] += dL_dh_next @ x.T\n",
    "    \n",
    "    def update_params(self, eps=1e-8):\n",
    "        for key in self.grads.keys():\n",
    "            self.grads[key] = torch.clamp(self.grads[key], -5, 5)\n",
    "            vars(self)[key + '_g_ada'] = self.gamma * vars(self)[key + '_g_ada'] + (1 - self.gamma) * self.grads[key] ** 2\n",
    "            vars(self)[key] -= self.eta * self.grads[key] / torch.sqrt(vars(self)[key + '_g_ada'] + eps)\n",
    "\n",
    "\n",
    "    def loss(self, p, y):\n",
    "        return - torch.sum(y.T @ torch.log(p))\n",
    "\n",
    "    def train(self, book_data, n_epochs=10):\n",
    "        n_iter = 0\n",
    "        smooth_loss = 0\n",
    "        best_loss = torch.inf\n",
    "        pbar = trange(n_epochs)\n",
    "\n",
    "        h = torch.zeros((self.m, 1)).to(self.device)\n",
    "        X = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "        Y = self.synthesize(h, X[:, 0].reshape(K, 1), 200)\n",
    "        print()\n",
    "        print(''.join([ind2char[torch.argmax(Y[:, i]).item()] for i in range(200)]))\n",
    "        print()\n",
    "\n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch {epoch}/{n_epochs}\")\n",
    "            self.e = 0\n",
    "            h = torch.zeros((self.m, 1)).to(self.device)\n",
    "            while self.e + self.seq_length <= len(book_data):\n",
    "                X_chars = book_data[self.e:self.e+self.seq_length]\n",
    "                Y_chars = book_data[self.e+1:self.e+self.seq_length+1]\n",
    "                X = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "                Y = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "                for i in range(self.seq_length):\n",
    "                    X[char2ind[X_chars[i]], i] = 1\n",
    "                    Y[char2ind[Y_chars[i]], i] = 1\n",
    "\n",
    "                H, P, loss = self.forward_pass(h, X, Y)\n",
    "                self.backward_pass(H, P, X, Y)\n",
    "                self.update_params()\n",
    "\n",
    "                smooth_loss = .999 * smooth_loss + .001 * loss if smooth_loss != 0 else loss\n",
    "                self.smooth_losses.append(smooth_loss)\n",
    "\n",
    "                if smooth_loss < best_loss:\n",
    "                    best_loss = smooth_loss\n",
    "                    self.V_best = self.V.clone()\n",
    "                    self.c_best = self.c.clone()\n",
    "                    self.W_best = self.W.clone()\n",
    "                    self.b_best = self.b.clone()\n",
    "                    self.U_best = self.U.clone()\n",
    "\n",
    "                h = H[:, -1].reshape(self.m, 1)\n",
    "\n",
    "                n_iter += 1\n",
    "                if n_iter % 1000 == 0:\n",
    "                    print(\n",
    "                        f\"Training - Iteration {n_iter} - Loss {smooth_loss}\")\n",
    "                    \n",
    "                if n_iter % 10000 == 0:\n",
    "                    Y = self.synthesize(h, X[:, 0].reshape(K, 1), 200)\n",
    "                    print()\n",
    "                    print(''.join([ind2char[torch.argmax(Y[:, i]).item()] for i in range(200)]))\n",
    "                    print()\n",
    "\n",
    "                self.e += self.seq_length\n",
    "\n",
    "        print(f\"Training done - Best loss: {best_loss}\")\n",
    "\n",
    "    def check_grads(self, book_data):\n",
    "        for key in self.grads.keys():\n",
    "            vars(self)[key].requires_grad = True\n",
    "\n",
    "        X_chars = book_data[:self.seq_length]\n",
    "        Y_chars = book_data[1:self.seq_length+1]\n",
    "        X = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "        Y = torch.zeros((K, self.seq_length)).to(self.device)\n",
    "        h0 = torch.zeros((self.m, 1)).to(self.device)\n",
    "        for i in range(self.seq_length):\n",
    "            X[char2ind[X_chars[i]], i] = 1\n",
    "            Y[char2ind[Y_chars[i]], i] = 1\n",
    "\n",
    "        H, P, loss = self.forward_pass(h0, X, Y)\n",
    "        self.backward_pass(H, P, X, Y)\n",
    "\n",
    "        for key in self.grads.keys():\n",
    "            vars(self)[key].retain_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        print(\"Checking gradients\")\n",
    "        with torch.no_grad():\n",
    "            for key in self.grads.keys():\n",
    "                diff = torch.norm(self.grads[key] - vars(self)[key].grad)\n",
    "                rel_err = diff / (torch.norm(self.grads[key]) + torch.norm(vars(self)[key].grad) + 1e-16)\n",
    "                print(f\"Relative error on {key}: {rel_err}\")\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.smooth_losses)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Smoothed loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradients\n",
      "Relative error on V: 2.5663803526754236e-08\n",
      "Relative error on c: 2.2815955347255112e-08\n",
      "Relative error on W: 3.914951918204679e-08\n",
      "Relative error on b: 3.6925428048562026e-08\n",
      "Relative error on U: 2.9062837469950864e-08\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "rnn.check_grads(book_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e508475a324be1a7f8086a04e54076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gZQa!MZo0,S4Qi vPYyu•\"z,.jgv9c:zFe6.mH\n",
      "dh4bqn\txMcH\txT:;VhrN/q-ozq!}COIDF4I2r1Fsz :LZn4M\n",
      "6I(gTdrGID4Qj!Xc!sT•j \t^,_DBxLQnrCVp.dG3deyehnwQ;\n",
      "FRmare(mbb6we/BEM\n",
      "'P1IxXcRu;B!Jx,'.üY!ic_ NpdR•N4y'gycNX17C1'U\n",
      "\n",
      "Training - Iteration 1000 - Loss 84.16536712646484\n",
      "Training - Iteration 2000 - Loss 70.28680419921875\n",
      "Training - Iteration 3000 - Loss 63.14775085449219\n",
      "Training - Iteration 4000 - Loss 60.042179107666016\n",
      "Training - Iteration 5000 - Loss 58.24436950683594\n",
      "Training - Iteration 6000 - Loss 57.577171325683594\n",
      "Training - Iteration 7000 - Loss 57.256935119628906\n",
      "Training - Iteration 8000 - Loss 54.97169876098633\n",
      "Training - Iteration 9000 - Loss 54.07980728149414\n",
      "Training - Iteration 10000 - Loss 53.84928512573242\n",
      "\n",
      "youndid , \"cad ?\"\n",
      "\"\n",
      "?ver. \n",
      "\"I gat tum them an moca pat jasa ket see stica beront toly.\n",
      "\"yary ak tharHe watf th ot't e's ilferly a mack oa miks at wh. Mr.\n",
      "\"Cs tuplle wormartcrofn then mach, enstp easte\n",
      "\n",
      "Training - Iteration 11000 - Loss 54.492706298828125\n",
      "Training - Iteration 12000 - Loss 53.8025016784668\n",
      "Training - Iteration 13000 - Loss 52.921302795410156\n",
      "Training - Iteration 14000 - Loss 52.138240814208984\n",
      "Training - Iteration 15000 - Loss 52.098609924316406\n",
      "Training - Iteration 16000 - Loss 51.04262924194336\n",
      "Training - Iteration 17000 - Loss 50.789119720458984\n",
      "Training - Iteration 18000 - Loss 50.86682891845703\n",
      "Training - Iteration 19000 - Loss 50.528160095214844\n",
      "Training - Iteration 20000 - Loss 49.59050369262695\n",
      "\n",
      "p an what ss d rim thag on of the laned lo y torer fimew bat rrecasking hid sitho, wnuch wnat hed a tired the \"rme that itter coucan, dow ind into the the . He fire whout, in agelone, lyet upt mingete\n",
      "\n",
      "Training - Iteration 21000 - Loss 49.1936149597168\n",
      "Training - Iteration 22000 - Loss 49.04066467285156\n",
      "Training - Iteration 23000 - Loss 49.08856964111328\n",
      "Training - Iteration 24000 - Loss 49.19815444946289\n",
      "Training - Iteration 25000 - Loss 49.25651931762695\n",
      "Training - Iteration 26000 - Loss 49.154808044433594\n",
      "Training - Iteration 27000 - Loss 48.86549377441406\n",
      "Training - Iteration 28000 - Loss 48.72015380859375\n",
      "Training - Iteration 29000 - Loss 48.08770751953125\n",
      "Training - Iteration 30000 - Loss 47.87261199951172\n",
      "\n",
      "ho, rest a bely.\n",
      "Harry featt Harry alllyand him, bobed every stire and a cleeped for there and grotes waswh therd were was galt 'eesarowly saring ghimming theme busicastef of -\"\n",
      "inching .vin shis he h\n",
      "\n",
      "Training - Iteration 31000 - Loss 48.19719314575195\n",
      "Training - Iteration 32000 - Loss 47.58187484741211\n",
      "Training - Iteration 33000 - Loss 47.78477478027344\n",
      "Training - Iteration 34000 - Loss 46.71748352050781\n",
      "Training - Iteration 35000 - Loss 46.421329498291016\n",
      "Training - Iteration 36000 - Loss 46.583248138427734\n",
      "Training - Iteration 37000 - Loss 46.898929595947266\n",
      "Training - Iteration 38000 - Loss 46.609954833984375\n",
      "Training - Iteration 39000 - Loss 45.886619567871094\n",
      "Training - Iteration 40000 - Loss 45.59083557128906\n",
      "\n",
      "hen comom nopidd they coldemort moodperburen, that eeding tion futa rish a folder whion finging faling reaning the ldeed if onesmid, thrihig an this froe.  e ffach as Harrys . . he oongw os thrteat gl\n",
      "\n",
      "Training - Iteration 41000 - Loss 44.87339782714844\n",
      "Training - Iteration 42000 - Loss 44.628089904785156\n",
      "Training - Iteration 43000 - Loss 45.08313751220703\n",
      "Training - Iteration 44000 - Loss 45.322242736816406\n",
      "Training - Iteration 45000 - Loss 46.813682556152344\n",
      "Training - Iteration 46000 - Loss 46.84153366088867\n",
      "Training - Iteration 47000 - Loss 47.10292434692383\n",
      "Training - Iteration 48000 - Loss 46.62874984741211\n",
      "Training - Iteration 49000 - Loss 46.71630859375\n",
      "Training - Iteration 50000 - Loss 47.49644470214844\n",
      "\n",
      "giwe thead and as the sofire.  alleras,\"  now at them.\n",
      "\"and iotrebin.  't  ibet hovee't and they have of aup, it to staarly that stoop who sarp ondem o\n",
      "\"m,arssty ta the\n",
      "\" maist you calks refecs breide\n",
      "\n",
      "Training - Iteration 51000 - Loss 48.62779235839844\n",
      "Training - Iteration 52000 - Loss 46.679935455322266\n",
      "Training - Iteration 53000 - Loss 45.18479919433594\n",
      "Training - Iteration 54000 - Loss 45.902652740478516\n",
      "Training - Iteration 55000 - Loss 48.150779724121094\n",
      "Training - Iteration 56000 - Loss 47.8475341796875\n",
      "Training - Iteration 57000 - Loss 47.246482849121094\n",
      "Training - Iteration 58000 - Loss 46.087947845458984\n",
      "Training - Iteration 59000 - Loss 47.08980178833008\n",
      "Training - Iteration 60000 - Loss 45.70494842529297\n",
      "\n",
      "ry had mont the enturnicted the simest a said pirt of the groothing luch an the drey reattore from that theehel,  eeen the greaking crosten.\n",
      "\n",
      "Hest thought curtllehind pifiuct as their from whistentena\n",
      "\n",
      "Training - Iteration 61000 - Loss 45.679805755615234\n",
      "Training - Iteration 62000 - Loss 45.14318084716797\n",
      "Training - Iteration 63000 - Loss 45.73246383666992\n",
      "Training - Iteration 64000 - Loss 45.211177825927734\n",
      "Training - Iteration 65000 - Loss 44.93919372558594\n",
      "Training - Iteration 66000 - Loss 44.30500793457031\n",
      "Training - Iteration 67000 - Loss 45.316593170166016\n",
      "Training - Iteration 68000 - Loss 45.4594841003418\n",
      "Training - Iteration 69000 - Loss 45.6883659362793\n",
      "Training - Iteration 70000 - Loss 45.728790283203125\n",
      "\n",
      "are you heflince he he's got and didn't a sive work. \". \" gim, sairssair us and endcyouse fico, and whse put have clossarts factly and  wentferseing cheare foiner stuw handboing this you, the snorng i\n",
      "\n",
      "Training - Iteration 71000 - Loss 45.66123580932617\n",
      "Training - Iteration 72000 - Loss 45.46077346801758\n",
      "Training - Iteration 73000 - Loss 44.9426383972168\n",
      "Training - Iteration 74000 - Loss 45.01942443847656\n",
      "Training - Iteration 75000 - Loss 44.56016540527344\n",
      "Training - Iteration 76000 - Loss 44.74465560913086\n",
      "Training - Iteration 77000 - Loss 44.635372161865234\n",
      "Training - Iteration 78000 - Loss 44.16997528076172\n",
      "Training - Iteration 79000 - Loss 43.10507583618164\n",
      "Training - Iteration 80000 - Loss 43.62993621826172\n",
      "\n",
      " weard now whowe out is youleor,\" said Harry lookedbeed aly. undle, umbeed.  encure aroom.  vell,\" gasilt.  re's ground.  Hagelys up.  said, along, you padned a bentn emcherrooked, The couiches.  eave\n",
      "\n",
      "Training - Iteration 81000 - Loss 43.1032829284668\n",
      "Training - Iteration 82000 - Loss 43.99140167236328\n",
      "Training - Iteration 83000 - Loss 43.1661491394043\n",
      "Training - Iteration 84000 - Loss 43.9370231628418\n",
      "Training - Iteration 85000 - Loss 42.55693817138672\n",
      "Training - Iteration 86000 - Loss 42.70869064331055\n",
      "Training - Iteration 87000 - Loss 42.911434173583984\n",
      "Training - Iteration 88000 - Loss 42.624969482421875\n",
      "Training - Iteration 89000 - Loss 44.223350524902344\n",
      "Training - Iteration 90000 - Loss 44.04008483886719\n",
      "\n",
      "f fther to at head Harryones bellow, at thoys bey ask 's. ttlonmed mip, scroust studen, in theo Hagrick at him withort be and and the ent recet with his bark thabghtkwen, and lippith straking audiser \n",
      "\n",
      "Training - Iteration 91000 - Loss 44.951438903808594\n",
      "Training - Iteration 92000 - Loss 44.60410690307617\n",
      "Training - Iteration 93000 - Loss 44.80373764038086\n",
      "Training - Iteration 94000 - Loss 45.58578109741211\n",
      "Training - Iteration 95000 - Loss 46.525787353515625\n",
      "Training - Iteration 96000 - Loss 45.2348747253418\n",
      "Training - Iteration 97000 - Loss 43.535133361816406\n",
      "Training - Iteration 98000 - Loss 44.221553802490234\n",
      "Training - Iteration 99000 - Loss 45.06977081298828\n",
      "Training - Iteration 100000 - Loss 45.647220611572266\n",
      "\n",
      "ituf langily as indotionaully. .\"  eled down the ot diom.  \"Hogwarts hood. . is a inky and returif down to choy, wh the befteIt who \"andyou and \"hbo the covllewtolpaling her anun, \"nthints.  evary fur\n",
      "\n",
      "Training - Iteration 101000 - Loss 45.73768615722656\n",
      "Training - Iteration 102000 - Loss 44.46675109863281\n",
      "Training - Iteration 103000 - Loss 45.50077819824219\n",
      "Training - Iteration 104000 - Loss 44.32328414916992\n",
      "Training - Iteration 105000 - Loss 43.53728103637695\n",
      "Training - Iteration 106000 - Loss 43.31972122192383\n",
      "Training - Iteration 107000 - Loss 44.2249641418457\n",
      "Training - Iteration 108000 - Loss 43.592681884765625\n",
      "Training - Iteration 109000 - Loss 43.54738998413086\n",
      "Training - Iteration 110000 - Loss 42.99917984008789\n",
      "\n",
      "mole mis e tasy walledwart to held don'e wald to him silled oppornt. . we and to cold, rut only dourden, in the crunces Hrriticald surnt of trow he lookingly out his warning fing a She looked Hagged t\n",
      "\n",
      "Training - Iteration 111000 - Loss 43.86808395385742\n",
      "Training - Iteration 112000 - Loss 43.876869201660156\n",
      "Training - Iteration 113000 - Loss 44.17940139770508\n",
      "Training - Iteration 114000 - Loss 44.348846435546875\n",
      "Training - Iteration 115000 - Loss 44.663578033447266\n",
      "Training - Iteration 116000 - Loss 43.65256118774414\n",
      "Training - Iteration 117000 - Loss 44.34965896606445\n",
      "Training - Iteration 118000 - Loss 44.191139221191406\n",
      "Training - Iteration 119000 - Loss 42.868343353271484\n",
      "Training - Iteration 120000 - Loss 43.62143325805664\n",
      "\n",
      " he was Hag your by they har prear- thing.  Harry mote to remoll culling the foot bigre theurming they suppes and inder, Harry.  The 'rous.  He one a loom Harry fapst, thouses wI hald whicled the bark\n",
      "\n",
      "Training - Iteration 121000 - Loss 43.26324462890625\n",
      "Training - Iteration 122000 - Loss 43.77729797363281\n",
      "Training - Iteration 123000 - Loss 41.93203353881836\n",
      "Training - Iteration 124000 - Loss 41.8697624206543\n",
      "Training - Iteration 125000 - Loss 42.21620178222656\n",
      "Training - Iteration 126000 - Loss 43.022003173828125\n",
      "Training - Iteration 127000 - Loss 42.652095794677734\n",
      "Training - Iteration 128000 - Loss 43.14579391479492\n",
      "Training - Iteration 129000 - Loss 41.55734634399414\n",
      "Training - Iteration 130000 - Loss 42.16350555419922\n",
      "\n",
      " iconter  co him mo scrinciend finssend the Dofric, she pader him for fake to thes forwe. .d you kninuters on latful, and Horrtilyfatlled his breever the hid\" pdilong stand invered.  There osse.  aven\n",
      "\n",
      "Training - Iteration 131000 - Loss 41.3719482421875\n",
      "Training - Iteration 132000 - Loss 41.88614273071289\n",
      "Training - Iteration 133000 - Loss 43.25374221801758\n",
      "Training - Iteration 134000 - Loss 43.080657958984375\n",
      "Training - Iteration 135000 - Loss 43.86515426635742\n",
      "Training - Iteration 136000 - Loss 43.615135192871094\n",
      "Training - Iteration 137000 - Loss 44.11693572998047\n",
      "Training - Iteration 138000 - Loss 44.10749053955078\n",
      "Training - Iteration 139000 - Loss 44.89736557006836\n",
      "Training - Iteration 140000 - Loss 45.61613464355469\n",
      "\n",
      "lore belyers over the one onet clange ulmpel hinters. Aitched foring the ruch plays could inxer sliochawia . ... on that he hamrs ares burely arthes in a chuffle, that, they the glyboar and the ontols\n",
      "\n",
      "Training - Iteration 141000 - Loss 43.53407287597656\n",
      "Training - Iteration 142000 - Loss 43.396995544433594\n",
      "Training - Iteration 143000 - Loss 43.89620590209961\n",
      "Training - Iteration 144000 - Loss 45.4287109375\n",
      "Training - Iteration 145000 - Loss 45.25564193725586\n",
      "Training - Iteration 146000 - Loss 44.14759063720703\n",
      "Training - Iteration 147000 - Loss 45.00495147705078\n",
      "Training - Iteration 148000 - Loss 44.13172912597656\n",
      "Training - Iteration 149000 - Loss 42.878047943115234\n",
      "Training - Iteration 150000 - Loss 42.80794906616211\n",
      "\n",
      "pplyom.blear he had bakn at might getely.  enoored a pidniented, into the poinds onthere cany tol ed to ontidets Hidd ctilled tham a batp enor, and vis 're ,\"\n",
      "lew as byssent forled hegaing held to ese\n",
      "\n",
      "Training - Iteration 151000 - Loss 43.272911071777344\n",
      "Training - Iteration 152000 - Loss 43.497474670410156\n",
      "Training - Iteration 153000 - Loss 42.7116813659668\n",
      "Training - Iteration 154000 - Loss 42.4664421081543\n",
      "Training - Iteration 155000 - Loss 42.72684860229492\n",
      "Training - Iteration 156000 - Loss 43.0705680847168\n",
      "Training - Iteration 157000 - Loss 43.20328140258789\n",
      "Training - Iteration 158000 - Loss 43.5980110168457\n",
      "Training - Iteration 159000 - Loss 43.618492126464844\n",
      "Training - Iteration 160000 - Loss 43.431541442871094\n",
      "\n",
      "hear the eing casing,\" said Harry saying sisct to unaul laaching for the poits efumon noga, gage diserth her saces bent aare sabooking ufter ploces bughaming and Ron, nder no is, of centhered the rean\n",
      "\n",
      "Training - Iteration 161000 - Loss 43.609561920166016\n",
      "Training - Iteration 162000 - Loss 43.10776901245117\n",
      "Training - Iteration 163000 - Loss 42.91869354248047\n",
      "Training - Iteration 164000 - Loss 43.521385192871094\n",
      "Training - Iteration 165000 - Loss 42.7712516784668\n",
      "Training - Iteration 166000 - Loss 43.41196823120117\n",
      "Training - Iteration 167000 - Loss 41.93093490600586\n",
      "Training - Iteration 168000 - Loss 42.06000900268555\n",
      "Training - Iteration 169000 - Loss 42.020477294921875\n",
      "Training - Iteration 170000 - Loss 42.6812744140625\n",
      "\n",
      "nto a trofe wannoting almon around. \"lawnurding himinister any scampie-d Hermione, and wowaret to rief tor the back anruggeced to get boungming it, could have bellewould hamroked to s artend of the da\n",
      "\n",
      "Training - Iteration 171000 - Loss 42.45512008666992\n",
      "Training - Iteration 172000 - Loss 42.139991760253906\n",
      "Training - Iteration 173000 - Loss 41.51817321777344\n",
      "Training - Iteration 174000 - Loss 41.20158386230469\n",
      "Training - Iteration 175000 - Loss 40.679874420166016\n",
      "Training - Iteration 176000 - Loss 41.18473815917969\n",
      "Training - Iteration 177000 - Loss 41.81159210205078\n",
      "Training - Iteration 178000 - Loss 43.1638069152832\n",
      "Training - Iteration 179000 - Loss 43.39036560058594\n",
      "Training - Iteration 180000 - Loss 43.33660888671875\n",
      "\n",
      "wros, though thebe berHardly singe out of the platleys spick to kill gat e onds saf accurely ough, prent lear this trosned run's hossm and \"\n",
      "nge, though have taashide twonskew.\n",
      "\"ous.  ,\"\n",
      ".... bac's mi\n",
      "\n",
      "Training - Iteration 181000 - Loss 43.65678405761719\n",
      "Training - Iteration 182000 - Loss 43.664798736572266\n",
      "Training - Iteration 183000 - Loss 44.66328430175781\n",
      "Training - Iteration 184000 - Loss 45.62302017211914\n",
      "Training - Iteration 185000 - Loss 43.42007827758789\n",
      "Training - Iteration 186000 - Loss 42.602420806884766\n",
      "Training - Iteration 187000 - Loss 43.14238357543945\n",
      "Training - Iteration 188000 - Loss 45.74425506591797\n",
      "Training - Iteration 189000 - Loss 45.25803756713867\n",
      "Training - Iteration 190000 - Loss 44.40128707885742\n",
      "\n",
      " lladgrsbry suddenfi, Harry besswere.\n",
      "\"I's whosed ndidpe gall freered.  . .  orterhandin, verA anlithed in crass at the stremes.  b o nan  turned she sway at his flood thoie boguse ol, haithater fight\n",
      "\n",
      "Training - Iteration 191000 - Loss 43.411949157714844\n",
      "Training - Iteration 192000 - Loss 44.02349853515625\n",
      "Training - Iteration 193000 - Loss 42.75511169433594\n",
      "Training - Iteration 194000 - Loss 42.72026443481445\n",
      "Training - Iteration 195000 - Loss 42.37104034423828\n",
      "Training - Iteration 196000 - Loss 43.158836364746094\n",
      "Training - Iteration 197000 - Loss 42.72265625\n",
      "Training - Iteration 198000 - Loss 42.323097229003906\n",
      "Training - Iteration 199000 - Loss 41.772090911865234\n",
      "Training - Iteration 200000 - Loss 42.99492645263672\n",
      "\n",
      "on a ticks, they with doom, a done, away keepering him in to foully at the sote to enting, all done of a fire, frowt him,\" Harry imuse what llo somewicle kes was yout they foundased lood.. . . \n",
      "\"powed\n",
      "\n",
      "Training - Iteration 201000 - Loss 43.255584716796875\n",
      "Training - Iteration 202000 - Loss 43.353370666503906\n",
      "Training - Iteration 203000 - Loss 43.43905258178711\n",
      "Training - Iteration 204000 - Loss 43.147979736328125\n",
      "Training - Iteration 205000 - Loss 43.36183166503906\n",
      "Training - Iteration 206000 - Loss 42.56105422973633\n",
      "Training - Iteration 207000 - Loss 42.87698745727539\n",
      "Training - Iteration 208000 - Loss 42.82284927368164\n",
      "Training - Iteration 209000 - Loss 42.629154205322266\n",
      "Training - Iteration 210000 - Loss 42.93378448486328\n",
      "\n",
      " staking down in, comewarricld.  though an emericly, they working samen clastmared for veruening whockly, thatch offor had behols.  eThe nower as up in the last lack all knowwore.  .   oth spright.  y\n",
      "\n",
      "Training - Iteration 211000 - Loss 41.74129104614258\n",
      "Training - Iteration 212000 - Loss 41.33926773071289\n",
      "Training - Iteration 213000 - Loss 41.7238883972168\n",
      "Training - Iteration 214000 - Loss 41.246856689453125\n",
      "Training - Iteration 215000 - Loss 41.87657165527344\n",
      "Training - Iteration 216000 - Loss 41.24686813354492\n",
      "Training - Iteration 217000 - Loss 41.88595199584961\n",
      "Training - Iteration 218000 - Loss 40.70573425292969\n",
      "Training - Iteration 219000 - Loss 40.775142669677734\n",
      "Training - Iteration 220000 - Loss 41.11033630371094\n",
      "\n",
      "t out is said Dumbledoreped acteste\" fitath to the wondd a goodd sleph your tomacll shiedgy of cuchoom, and the netterwisped failans of the demblicaur creat blays wither plans whoweren as then certare\n",
      "\n",
      "Training - Iteration 221000 - Loss 41.177249908447266\n",
      "Training - Iteration 222000 - Loss 42.71889114379883\n",
      "Training - Iteration 223000 - Loss 42.234432220458984\n",
      "Training - Iteration 224000 - Loss 43.17972183227539\n",
      "Training - Iteration 225000 - Loss 42.77542495727539\n",
      "Training - Iteration 226000 - Loss 43.46770477294922\n",
      "Training - Iteration 227000 - Loss 44.14341354370117\n",
      "Training - Iteration 228000 - Loss 45.303470611572266\n",
      "Training - Iteration 229000 - Loss 43.80625915527344\n",
      "Training - Iteration 230000 - Loss 41.85721206665039\n",
      "\n",
      "re, heepped loucting that she keast lost mined. \"What and the balfieave you wureash.\n",
      "\"able,  the mispveed.\n",
      "\"link frow  oddred though Mr. Cedood.\n",
      " enty bet ones off conous lass, eldn't Hogan were such \n",
      "\n",
      "Training - Iteration 231000 - Loss 42.7519416809082\n",
      "Training - Iteration 232000 - Loss 43.589599609375\n",
      "Training - Iteration 233000 - Loss 44.248355865478516\n",
      "Training - Iteration 234000 - Loss 44.44510269165039\n",
      "Training - Iteration 235000 - Loss 43.27756881713867\n",
      "Training - Iteration 236000 - Loss 44.116615295410156\n",
      "Training - Iteration 237000 - Loss 42.37196731567383\n",
      "Training - Iteration 238000 - Loss 42.320030212402344\n",
      "Training - Iteration 239000 - Loss 41.83734893798828\n",
      "Training - Iteration 240000 - Loss 42.67148208618164\n",
      "\n",
      "saw in of allitted be,\" said seet of ope it impot Mavifles.\n",
      "\"noo,  timbee shands his face of alrounding -\"\n",
      "\"Harry hood.  \" He wand heve to endesed from. \"lo-- keele ander Ron a rifolly rowes were, he \n",
      "\n",
      "Training - Iteration 241000 - Loss 42.079410552978516\n",
      "Training - Iteration 242000 - Loss 42.02619552612305\n",
      "Training - Iteration 243000 - Loss 41.6475715637207\n",
      "Training - Iteration 244000 - Loss 42.877593994140625\n",
      "Training - Iteration 245000 - Loss 42.512569427490234\n",
      "Training - Iteration 246000 - Loss 42.899658203125\n",
      "Training - Iteration 247000 - Loss 42.98722457885742\n",
      "Training - Iteration 248000 - Loss 43.777828216552734\n",
      "Training - Iteration 249000 - Loss 42.8221321105957\n",
      "Training - Iteration 250000 - Loss 43.13797378540039\n",
      "\n",
      "pericertly.  \"I'm ownane.r sound you enothes \n",
      "apwast brabblefter down trying ner coached gst to and o thoed the batters plo unarthing too bucked it.  down to done Harry behond stom his betained Moody,\n",
      "\n",
      "Training - Iteration 251000 - Loss 43.01680374145508\n",
      "Training - Iteration 252000 - Loss 42.01564025878906\n",
      "Training - Iteration 253000 - Loss 42.60578536987305\n",
      "Training - Iteration 254000 - Loss 42.197509765625\n",
      "Training - Iteration 255000 - Loss 42.35663986206055\n",
      "Training - Iteration 256000 - Loss 40.799625396728516\n",
      "Training - Iteration 257000 - Loss 40.83938217163086\n",
      "Training - Iteration 258000 - Loss 40.750423431396484\n",
      "Training - Iteration 259000 - Loss 41.81243133544922\n",
      "Training - Iteration 260000 - Loss 41.40943908691406\n",
      "\n",
      "ns his hang himselt, suddenly toward look.\n",
      "then he heakp oing than him.  Harry from the to kntood raiches.  He so couldnefess of was puttward it flout of the \" said Harry, he stonk the tlisenting look\n",
      "\n",
      "Training - Iteration 261000 - Loss 41.902400970458984\n",
      "Training - Iteration 262000 - Loss 40.3078498840332\n",
      "Training - Iteration 263000 - Loss 40.7401237487793\n",
      "Training - Iteration 264000 - Loss 40.40521240234375\n",
      "Training - Iteration 265000 - Loss 40.621421813964844\n",
      "Training - Iteration 266000 - Loss 42.479820251464844\n",
      "Training - Iteration 267000 - Loss 42.05253982543945\n",
      "Training - Iteration 268000 - Loss 42.68230438232422\n",
      "Training - Iteration 269000 - Loss 42.39089584350586\n",
      "Training - Iteration 270000 - Loss 43.01866149902344\n",
      "\n",
      "eating a'me for inule Broat.... Wone bli.  or wish arcabottly to Hagridly.  thadped tobors.  \"-said Mugges.  \"for the other, wants for the cust Mrs.ne nsing of vory hands coo donecters aresHalward.   \n",
      "\n",
      "Training - Iteration 271000 - Loss 43.35383224487305\n",
      "Training - Iteration 272000 - Loss 44.15317916870117\n",
      "Training - Iteration 273000 - Loss 44.470298767089844\n",
      "Training - Iteration 274000 - Loss 42.49142074584961\n",
      "Training - Iteration 275000 - Loss 42.59537887573242\n",
      "Training - Iteration 276000 - Loss 42.956336975097656\n",
      "Training - Iteration 277000 - Loss 44.47268295288086\n",
      "Training - Iteration 278000 - Loss 44.676483154296875\n",
      "Training - Iteration 279000 - Loss 42.987831115722656\n",
      "Training - Iteration 280000 - Loss 44.20317077636719\n",
      "\n",
      "on not into the widhaaupoyosermight say wenling the pointe to done find at the resoppiong to dratten eater. r. s the from that the atouthtly.\"\n",
      "I'd had been byes twers preeskabor. ,  of person boying a\n",
      "\n",
      "Training - Iteration 281000 - Loss 43.067543029785156\n",
      "Training - Iteration 282000 - Loss 42.05693435668945\n",
      "Training - Iteration 283000 - Loss 41.81354522705078\n",
      "Training - Iteration 284000 - Loss 42.4083137512207\n",
      "Training - Iteration 285000 - Loss 42.30341339111328\n",
      "Training - Iteration 286000 - Loss 41.66157531738281\n",
      "Training - Iteration 287000 - Loss 41.48760223388672\n",
      "Training - Iteration 288000 - Loss 42.18888473510742\n",
      "Training - Iteration 289000 - Loss 42.5184326171875\n",
      "Training - Iteration 290000 - Loss 42.41029357910156\n",
      "\n",
      "keeling is from it was aim ers, deak gightend and happed abas, is heaving then ust with trmay ones, musting room for the bbokt, curing her eyes weving winly said.\n",
      "\"hanyma toglase, of could up at Hight\n",
      "\n",
      "Training - Iteration 291000 - Loss 43.21259689331055\n",
      "Training - Iteration 292000 - Loss 42.53424072265625\n",
      "Training - Iteration 293000 - Loss 42.585914611816406\n",
      "Training - Iteration 294000 - Loss 42.88607406616211\n",
      "Training - Iteration 295000 - Loss 42.338382720947266\n",
      "Training - Iteration 296000 - Loss 42.10711669921875\n",
      "Training - Iteration 297000 - Loss 42.771583557128906\n",
      "Training - Iteration 298000 - Loss 41.941043853759766\n",
      "Training - Iteration 299000 - Loss 42.55460739135742\n",
      "Training - Iteration 300000 - Loss 41.0235595703125\n",
      "\n",
      "n trrike, doingeds trouted.\n",
      "\"riuntly arieved down nowd he day a mumbledil to hell sendered the staling in very cauggrappening make for dinched ac singerry.\n",
      "Hagrid said.\n",
      "\"widn gronni?\"\n",
      "\", I'll himselfa\n",
      "\n",
      "Training - Iteration 301000 - Loss 41.18221664428711\n",
      "Training - Iteration 302000 - Loss 41.219417572021484\n",
      "Training - Iteration 303000 - Loss 41.81395721435547\n",
      "Training - Iteration 304000 - Loss 41.503475189208984\n",
      "Training - Iteration 305000 - Loss 41.68958282470703\n",
      "Training - Iteration 306000 - Loss 40.39607238769531\n",
      "Training - Iteration 307000 - Loss 40.524417877197266\n",
      "Training - Iteration 308000 - Loss 39.82814407348633\n",
      "Training - Iteration 309000 - Loss 40.31133270263672\n",
      "Training - Iteration 310000 - Loss 41.20134735107422\n",
      "\n",
      "ehen the powley sagring that as at undires looked, and made to kempitsor.houred you don't wabtuets free.\"\n",
      "\" winery eay,\" said he owl in head at plengroum,\" said fttings about ouches acy fit he was eke\n",
      "\n",
      "Training - Iteration 311000 - Loss 42.213714599609375\n",
      "Training - Iteration 312000 - Loss 42.83252716064453\n",
      "Training - Iteration 313000 - Loss 42.29640197753906\n",
      "Training - Iteration 314000 - Loss 42.79154586791992\n",
      "Training - Iteration 315000 - Loss 43.05995559692383\n",
      "Training - Iteration 316000 - Loss 43.93285369873047\n",
      "Training - Iteration 317000 - Loss 45.00218963623047\n",
      "Training - Iteration 318000 - Loss 42.470314025878906\n",
      "Training - Iteration 319000 - Loss 42.08665466308594\n",
      "Training - Iteration 320000 - Loss 42.127071380615234\n",
      "\n",
      "ewars.\n",
      "I wanding to the crowded atther they what think be headdce, and thatheis. \"Illonaiy dasced of the \n",
      "mbeting himatiol ofling in breatter, sorging in then their floth daining eyes. ..  on any lut \n",
      "\n",
      "Training - Iteration 321000 - Loss 44.83552551269531\n",
      "Training - Iteration 322000 - Loss 44.512176513671875\n",
      "Training - Iteration 323000 - Loss 43.420101165771484\n",
      "Training - Iteration 324000 - Loss 42.85487365722656\n",
      "Training - Iteration 325000 - Loss 43.54377365112305\n",
      "Training - Iteration 326000 - Loss 42.35660171508789\n",
      "Training - Iteration 327000 - Loss 41.93196487426758\n",
      "Training - Iteration 328000 - Loss 42.14362716674805\n",
      "Training - Iteration 329000 - Loss 42.41305160522461\n",
      "Training - Iteration 330000 - Loss 41.804115295410156\n",
      "\n",
      "bass so would him, than Hogons wart of a stopight inter the the pahe nothin, and Harry, yout to dearly, you coling heurwise.  The ckut in malk to large along her a barks to dpernticloun,  at och.\n",
      "\"hur\n",
      "\n",
      "Training - Iteration 331000 - Loss 41.58281707763672\n",
      "Training - Iteration 332000 - Loss 41.473731994628906\n",
      "Training - Iteration 333000 - Loss 42.34175491333008\n",
      "Training - Iteration 334000 - Loss 42.79666519165039\n",
      "Training - Iteration 335000 - Loss 42.660709381103516\n",
      "Training - Iteration 336000 - Loss 42.9349479675293\n",
      "Training - Iteration 337000 - Loss 42.53530502319336\n",
      "Training - Iteration 338000 - Loss 42.92880630493164\n",
      "Training - Iteration 339000 - Loss 42.0881233215332\n",
      "Training - Iteration 340000 - Loss 42.38337326049805\n",
      "\n",
      "athed a\" creath rigs of down the elched bath, thair.  He said  oter.  piosing whicked when eat's  the ence surting, row seenger starur, it was be wasched, and lands plashed two the pain, their lldared\n",
      "\n",
      "Training - Iteration 341000 - Loss 42.806636810302734\n",
      "Training - Iteration 342000 - Loss 42.05967712402344\n",
      "Training - Iteration 343000 - Loss 42.462646484375\n",
      "Training - Iteration 344000 - Loss 41.16365051269531\n",
      "Training - Iteration 345000 - Loss 40.63303756713867\n",
      "Training - Iteration 346000 - Loss 41.092796325683594\n",
      "Training - Iteration 347000 - Loss 41.19959259033203\n",
      "Training - Iteration 348000 - Loss 41.15575408935547\n",
      "Training - Iteration 349000 - Loss 40.81504440307617\n",
      "Training - Iteration 350000 - Loss 41.306175231933594\n",
      "\n",
      "y thim. . . . was go voiced him need Harry w\"lo to roes weached at the unnered in, was who haspulishape, didn,\" he roils wh what Dumbledore that his lookn't the guorts eyest that were beay reavling, n\n",
      "\n",
      "Training - Iteration 351000 - Loss 40.481849670410156\n",
      "Training - Iteration 352000 - Loss 40.31051254272461\n",
      "Training - Iteration 353000 - Loss 40.530433654785156\n",
      "Training - Iteration 354000 - Loss 40.53004837036133\n",
      "Training - Iteration 355000 - Loss 42.49586486816406\n",
      "Training - Iteration 356000 - Loss 42.143741607666016\n",
      "Training - Iteration 357000 - Loss 42.91714859008789\n",
      "Training - Iteration 358000 - Loss 42.31986618041992\n",
      "Training - Iteration 359000 - Loss 42.78192901611328\n",
      "Training - Iteration 360000 - Loss 43.752689361572266\n",
      "\n",
      "iments with the blagar sparosont for -\"\n",
      "win an eh, \"Airowl were so he would all by ense rought and seemed thatered of the some stunding that, they donn a mes, pitilal switting them they wats,\" said ne\n",
      "\n",
      "Training - Iteration 361000 - Loss 45.11083221435547\n",
      "Training - Iteration 362000 - Loss 43.3720588684082\n",
      "Training - Iteration 363000 - Loss 41.41679763793945\n",
      "Training - Iteration 364000 - Loss 42.29684829711914\n",
      "Training - Iteration 365000 - Loss 43.666831970214844\n",
      "Training - Iteration 366000 - Loss 43.951377868652344\n",
      "Training - Iteration 367000 - Loss 44.00035858154297\n",
      "Training - Iteration 368000 - Loss 42.911033630371094\n",
      "Training - Iteration 369000 - Loss 43.728389739990234\n",
      "Training - Iteration 370000 - Loss 41.99736785888672\n",
      "\n",
      "Ged one you know put it was a laired.\n",
      "The durlwnse wasting a laod usli gamrtin, comprin leadigis had madnousle rear before of you.  \"went of his bent called earming to the take all refutain.  \"rhif in\n",
      "\n",
      "Training - Iteration 371000 - Loss 41.73365020751953\n",
      "Training - Iteration 372000 - Loss 41.469398498535156\n",
      "Training - Iteration 373000 - Loss 42.4332389831543\n",
      "Training - Iteration 374000 - Loss 41.777992248535156\n",
      "Training - Iteration 375000 - Loss 41.652565002441406\n",
      "Training - Iteration 376000 - Loss 41.20950698852539\n",
      "Training - Iteration 377000 - Loss 42.31824493408203\n",
      "Training - Iteration 378000 - Loss 42.48164749145508\n",
      "Training - Iteration 379000 - Loss 42.528072357177734\n",
      "Training - Iteration 380000 - Loss 42.769962310791016\n",
      "\n",
      "mauly, voce uising, and 'r. \"mean sfow contine when werring to \"but shelens with back their moy to same into the shar opening at Harry go doncelly, before.\n",
      "\"fiurrayed these horm\n",
      "on eangly to  orle, \"H\n",
      "\n",
      "Training - Iteration 381000 - Loss 43.11709213256836\n",
      "Training - Iteration 382000 - Loss 42.78904342651367\n",
      "Training - Iteration 383000 - Loss 42.57789993286133\n",
      "Training - Iteration 384000 - Loss 42.75580596923828\n",
      "Training - Iteration 385000 - Loss 41.950401306152344\n",
      "Training - Iteration 386000 - Loss 42.351036071777344\n",
      "Training - Iteration 387000 - Loss 41.61400604248047\n",
      "Training - Iteration 388000 - Loss 41.71523666381836\n",
      "Training - Iteration 389000 - Loss 40.41233825683594\n",
      "Training - Iteration 390000 - Loss 40.89033126831055\n",
      "\n",
      "you sele of his foelimening to nold intend him mattily.  him, gonn thear know soHce be you in nurione neress fresway, throoc hebered naminbbach to it unan down through the clut like don its and churs \n",
      "\n",
      "Training - Iteration 391000 - Loss 40.388092041015625\n",
      "Training - Iteration 392000 - Loss 41.22780227661133\n",
      "Training - Iteration 393000 - Loss 40.708011627197266\n",
      "Training - Iteration 394000 - Loss 41.786949157714844\n",
      "Training - Iteration 395000 - Loss 40.070735931396484\n",
      "Training - Iteration 396000 - Loss 40.49160385131836\n",
      "Training - Iteration 397000 - Loss 40.056785583496094\n",
      "Training - Iteration 398000 - Loss 40.225311279296875\n",
      "Training - Iteration 399000 - Loss 42.15459442138672\n",
      "Training - Iteration 400000 - Loss 41.6556396484375\n",
      "\n",
      "ng hapet dangersoy folled and pulliry a fingers a licrable begained aroubly leamins, that pote to comy to listen poovery his let to enough Harry, and 'd aney not, old more donward, chathes of at him, \n",
      "\n",
      "Training - Iteration 401000 - Loss 42.688175201416016\n",
      "Training - Iteration 402000 - Loss 42.21400451660156\n",
      "Training - Iteration 403000 - Loss 42.581058502197266\n",
      "Training - Iteration 404000 - Loss 43.22111511230469\n",
      "Training - Iteration 405000 - Loss 43.62782669067383\n",
      "Training - Iteration 406000 - Loss 43.5626335144043\n",
      "Training - Iteration 407000 - Loss 41.95546340942383\n",
      "Training - Iteration 408000 - Loss 42.1490364074707\n",
      "Training - Iteration 409000 - Loss 42.89617919921875\n",
      "Training - Iteration 410000 - Loss 43.897705078125\n",
      "\n",
      "ld chatberchoun.  furge gotnbed into hay attilys ie old me of could fast facing this to ppareenly, avorles. . . thata nossed ones \"was name wien'fled there wor cighide to human lurk of all , his eachi\n",
      "\n",
      "Training - Iteration 411000 - Loss 44.27483367919922\n",
      "Training - Iteration 412000 - Loss 42.74089813232422\n",
      "Training - Iteration 413000 - Loss 43.86741256713867\n",
      "Training - Iteration 414000 - Loss 42.483394622802734\n",
      "Training - Iteration 415000 - Loss 41.53578186035156\n",
      "Training - Iteration 416000 - Loss 41.413597106933594\n",
      "Training - Iteration 417000 - Loss 42.14058303833008\n",
      "Training - Iteration 418000 - Loss 41.76007843017578\n",
      "Training - Iteration 419000 - Loss 41.54547119140625\n",
      "Training - Iteration 420000 - Loss 41.118133544921875\n",
      "\n",
      "ghtant sirsning up enerring eyouts a marreng, very strider rebior, so tin.  you with the tans, could fining in a simmoring.  thick threen that nig that had turning mank, Harry triamed him getwol no th\n",
      "\n",
      "Training - Iteration 421000 - Loss 41.954078674316406\n",
      "Training - Iteration 422000 - Loss 42.16380310058594\n",
      "Training - Iteration 423000 - Loss 42.332550048828125\n",
      "Training - Iteration 424000 - Loss 42.752784729003906\n",
      "Training - Iteration 425000 - Loss 42.32307434082031\n",
      "Training - Iteration 426000 - Loss 42.070430755615234\n",
      "Training - Iteration 427000 - Loss 42.959503173828125\n",
      "Training - Iteration 428000 - Loss 42.23268127441406\n",
      "Training - Iteration 429000 - Loss 41.72644805908203\n",
      "Training - Iteration 430000 - Loss 42.311580657958984\n",
      "\n",
      "he righ to sning his Tgornwardy.  eats. . . . . . the ruckar, right of his hooced saye of ttan armago back to hapred.\"\n",
      "Harry could as uddinn an when of him nots my the one looky out his possone of the\n",
      "\n",
      "Training - Iteration 431000 - Loss 41.70508575439453\n",
      "Training - Iteration 432000 - Loss 42.17967987060547\n",
      "Training - Iteration 433000 - Loss 40.579288482666016\n",
      "Training - Iteration 434000 - Loss 40.729522705078125\n",
      "Training - Iteration 435000 - Loss 40.81182861328125\n",
      "Training - Iteration 436000 - Loss 41.402488708496094\n",
      "Training - Iteration 437000 - Loss 41.037841796875\n",
      "Training - Iteration 438000 - Loss 41.61568832397461\n",
      "Training - Iteration 439000 - Loss 39.98061752319336\n",
      "Training - Iteration 440000 - Loss 40.64213943481445\n",
      "\n",
      "n into mocrion, and down't know, and so ngriary ortering.\n",
      "\"?\"  discable of father mally, an in mus ost we Tournave it, when troind of him the othered oning as mightes, his feath my as thoushed a tack,\n",
      "\n",
      "Training - Iteration 441000 - Loss 39.621734619140625\n",
      "Training - Iteration 442000 - Loss 39.97770309448242\n",
      "Training - Iteration 443000 - Loss 40.95890808105469\n",
      "Training done - Best loss: 39.54579544067383\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "rnn.train(book_data, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnn.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e into the ground, and  uiry his well, trymbled beath and Who saided them hag withail a please enease wolked yourmoned into Horut. ovither were hatched . ... we look.  His eyes rovane, how his eveningacketyns, still beay from the porion He that holding me . . . went .\"  wadove bean befant .  adong to wailedy will.  He witch it.  I couck downed.  Harry to besher werry if he was rempalling me. , had about on read and that to he would him.  re indemation.  ye a drowing  Iat nlight the poonlytroy, at Dumbledore me shoon.  \"I kninged, percmaster.\"\n",
      "Harry teath Harry, and benafing the hat \n",
      "\"b.  would he feight me.  \"its at your's och to deSt you winnet this face ragged,\" said Dumbledore, he acondive.  as theel time . . . me bedork, thince.  on to him, stairIn right on but what he was will loiddre wandelf and bey\" she was doing at the rectioc not, leggs as it.  Harry.\n",
      "\"\" wiHa wiely in a dow a lined and chasice now along whelessinged them he teunded eps and think you gous, he plowered chsend be\n"
     ]
    }
   ],
   "source": [
    "X = torch.zeros((K, 1)).to(device)\n",
    "h = torch.zeros((rnn.m, 1)).to(device)\n",
    "Y = rnn.synthesize(h, X, 1000, best=True)\n",
    "print(''.join([ind2char[torch.argmax(Y[:, i]).item()] for i in range(1000)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
